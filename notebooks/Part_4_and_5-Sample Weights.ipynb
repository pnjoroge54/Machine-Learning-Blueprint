{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "883cfcc8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "607009b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-03 23:55:05.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mafml\u001b[0m:\u001b[36msetup_jupyter\u001b[0m:\u001b[36m281\u001b[0m - \u001b[1mSetting up AFML for Jupyter notebook...\u001b[0m\n",
      "\u001b[32m2025-11-03 23:55:05.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mafml.cache\u001b[0m:\u001b[36msetup_jupyter_cache\u001b[0m:\u001b[36m583\u001b[0m - \u001b[1mSetting up cache for Jupyter notebook...\u001b[0m\n",
      "\u001b[32m2025-11-03 23:55:05.599\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mafml.cache\u001b[0m:\u001b[36m_configure_numba\u001b[0m:\u001b[36m59\u001b[0m - \u001b[34m\u001b[1mNumba cache configured: C:\\Users\\JoeN\\AppData\\Local\\afml\\afml\\Cache\\numba_cache\u001b[0m\n",
      "\u001b[32m2025-11-03 23:55:05.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mafml.cache\u001b[0m:\u001b[36minitialize_cache_system\u001b[0m:\u001b[36m308\u001b[0m - \u001b[1mAFML cache system initialized:\u001b[0m\n",
      "\u001b[32m2025-11-03 23:55:05.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mafml.cache\u001b[0m:\u001b[36minitialize_cache_system\u001b[0m:\u001b[36m309\u001b[0m - \u001b[1m  Joblib cache: C:\\Users\\JoeN\\AppData\\Local\\afml\\afml\\Cache\\joblib_cache\u001b[0m\n",
      "\u001b[32m2025-11-03 23:55:05.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mafml.cache\u001b[0m:\u001b[36minitialize_cache_system\u001b[0m:\u001b[36m310\u001b[0m - \u001b[1m  Numba cache: C:\\Users\\JoeN\\AppData\\Local\\afml\\afml\\Cache\\numba_cache\u001b[0m\n",
      "\u001b[32m2025-11-03 23:55:05.611\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mafml.cache\u001b[0m:\u001b[36minitialize_cache_system\u001b[0m:\u001b[36m316\u001b[0m - \u001b[1m  Loaded stats: 3 functions, 93.8% hit rate\u001b[0m\n",
      "\u001b[32m2025-11-03 23:55:05.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mafml.cache.mlflow_integration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m54\u001b[0m - \u001b[1mMLflow tracking enabled: experiment=jupyter_experiments\u001b[0m\n",
      "\u001b[32m2025-11-03 23:55:05.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mafml.cache\u001b[0m:\u001b[36msetup_jupyter_cache\u001b[0m:\u001b[36m613\u001b[0m - \u001b[1m✅ Jupyter cache ready!\u001b[0m\n",
      "\u001b[32m2025-11-03 23:55:05.633\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mafml.cache\u001b[0m:\u001b[36msetup_jupyter_cache\u001b[0m:\u001b[36m614\u001b[0m - \u001b[1m   Use helpers: cache_status(), print_health(), optimize()\u001b[0m\n",
      "\u001b[32m2025-11-03 23:55:05.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mafml\u001b[0m:\u001b[36msetup_jupyter\u001b[0m:\u001b[36m304\u001b[0m - \u001b[1m✅ AFML Jupyter environment ready!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n",
      "Working Dir: c:\\Users\\JoeN\\Documents\\GitHub\\Machine-Learning-Blueprint\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# --- Extension Setup ---\n",
    "%load_ext autoreload\n",
    "%load_ext line_profiler\n",
    "%autoreload 3 -p\n",
    "\n",
    "# --- Module Imports ---\n",
    "import sys\n",
    "sys.path.append(\"..\")  # Adjust if your afml repo is nested differently\n",
    "\n",
    "# --- Autoreload Target ---\n",
    "%aimport afml\n",
    "\n",
    "# --- AFML Initialization ---\n",
    "# Setup with auto-reload enabled\n",
    "import afml\n",
    "\n",
    "# Enhanced setup with all features\n",
    "components = afml.setup_jupyter(\n",
    "    enable_mlflow=True,      # Set True if you have mlflow installed\n",
    "    enable_monitoring=True,    # Cache analytics\n",
    ")\n",
    "\n",
    "# --- Environment Diagnostics ---\n",
    "from pathlib import Path\n",
    "print(f\"Working Dir: {Path.cwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import winsound\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import MetaTrader5 as mt5\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    BaggingClassifier,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from afml.cross_validation import (\n",
    "    PurgedKFold,\n",
    "    PurgedSplit,\n",
    "    analyze_cross_val_scores,\n",
    "    ml_cross_val_score,\n",
    "    probability_weighted_accuracy,\n",
    ")\n",
    "from afml.cross_validation.scoring import probability_weighted_accuracy\n",
    "from afml.data_structures.bars import *\n",
    "from afml.ensemble.sb_bagging import (\n",
    "    SequentiallyBootstrappedBaggingClassifier,\n",
    "    compute_custom_oob_metrics,\n",
    "    estimate_ensemble_size,\n",
    ")\n",
    "from afml.labeling.triple_barrier import (\n",
    "    add_vertical_barrier,\n",
    "    get_event_weights,\n",
    "    triple_barrier_labels,\n",
    ")\n",
    "from afml.mt5.load_data import get_bars, get_ticks, login_mt5, save_data_to_parquet\n",
    "from afml.sample_weights.optimized_attribution import (\n",
    "    get_weights_by_time_decay_optimized,\n",
    ")\n",
    "\n",
    "# from afml.sampling import get_ind_mat_average_uniqueness, get_ind_matrix, seq_bootstrap\n",
    "from afml.strategies import (\n",
    "    BollingerStrategy,\n",
    "    MACrossoverStrategy,\n",
    "    create_bollinger_features,\n",
    "    get_entries,\n",
    "    ma_crossover_feature_engine,\n",
    ")\n",
    "from afml.util import get_daily_vol, value_counts_data\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41127511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CACHE HEALTH REPORT\n",
      "======================================================================\n",
      "\n",
      "Overall Statistics:\n",
      "  Total Functions:     3\n",
      "  Total Calls:         160\n",
      "  Overall Hit Rate:    93.8%\n",
      "  Total Cache Size:    0.00 MB\n",
      "\n",
      "Top Performers (by hit rate):\n",
      "  1. triple_barrier_labels: 98.0% (153 calls)\n",
      "  2. create_bollinger_features: 0.0% (2 calls)\n",
      "  3. get_event_weights: 0.0% (5 calls)\n",
      "\n",
      "Worst Performers (by hit rate):\n",
      "  1. triple_barrier_labels: 98.0% (153 calls)\n",
      "  2. create_bollinger_features: 0.0% (2 calls)\n",
      "  3. get_event_weights: 0.0% (5 calls)\n",
      "\n",
      "Recommendations:\n",
      "  1. Excellent hit rate (>90%)! Cache system is performing well.\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "function",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "calls",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hits",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "misses",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hit_rate",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "avg_time_ms",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cache_size_mb",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "last_access",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "ac0ea995-fb2a-4f03-82f9-a9a486661f85",
       "rows": [
        [
         "1",
         "afml.labeling.triple_barrier.triple_barrier_labels",
         "153",
         "150",
         "3",
         "98.0%",
         "N/A",
         "N/A",
         "N/A"
        ],
        [
         "2",
         "afml.labeling.triple_barrier.get_event_weights",
         "5",
         "0",
         "5",
         "0.0%",
         "N/A",
         "N/A",
         "N/A"
        ],
        [
         "0",
         "afml.strategies.bollinger_features.create_bollinger_features",
         "2",
         "0",
         "2",
         "0.0%",
         "N/A",
         "N/A",
         "N/A"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function</th>\n",
       "      <th>calls</th>\n",
       "      <th>hits</th>\n",
       "      <th>misses</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>avg_time_ms</th>\n",
       "      <th>cache_size_mb</th>\n",
       "      <th>last_access</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afml.labeling.triple_barrier.triple_barrier_la...</td>\n",
       "      <td>153</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>98.0%</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afml.labeling.triple_barrier.get_event_weights</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afml.strategies.bollinger_features.create_boll...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            function  calls  hits  misses  \\\n",
       "1  afml.labeling.triple_barrier.triple_barrier_la...    153   150       3   \n",
       "2     afml.labeling.triple_barrier.get_event_weights      5     0       5   \n",
       "0  afml.strategies.bollinger_features.create_boll...      2     0       2   \n",
       "\n",
       "  hit_rate avg_time_ms cache_size_mb last_access  \n",
       "1    98.0%         N/A           N/A         N/A  \n",
       "2     0.0%         N/A           N/A         N/A  \n",
       "0     0.0%         N/A           N/A         N/A  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add to your startup script or notebook\n",
    "from afml.cache import get_cache_efficiency_report, print_cache_health\n",
    "\n",
    "# Check cache health anytime\n",
    "print_cache_health()\n",
    "\n",
    "# Find functions with low hit rates or high call counts\n",
    "df = get_cache_efficiency_report()\n",
    "df.sort_values('calls', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160822e7",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f257b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = \"EURUSD\"\n",
    "start_date, end_date = \"2018-01-01\", \"2024-12-31\"\n",
    "sample_start, sample_end = start_date, \"2023-12-31\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9129c536",
   "metadata": {},
   "source": [
    "## 2. Bollinger Band Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f960964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_timeframe = \"M5\"\n",
    "file = Path(r\"..\\data\\EURUSD_M5_time_2018-01-01-2024-12-31.parq\")\n",
    "bb_time_bars = pd.read_parquet(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_period, bb_std = 20, 2 # Bollinger Band parameters\n",
    "bb_strategy = BollingerStrategy(window=bb_period, num_std=bb_std)\n",
    "bb_lookback = 10\n",
    "bb_pt_barrier, bb_sl_barrier, bb_time_horizon = (1, 2, dict(days=1))\n",
    "min_ret = 5e-5\n",
    "bb_vol_multiplier = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83dccff",
   "metadata": {},
   "source": [
    "### Time-Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d9465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bollinger_w20_std2 Signals:\n",
      "\n",
      "        count  proportion\n",
      "side                     \n",
      " 0    373,536    0.842213\n",
      "-1     35,095    0.079129\n",
      " 1     34,886    0.078658\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-03 23:55:11.985\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mafml.filters.filters\u001b[0m:\u001b[36mcusum_filter\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1m19,458 CUSUM-filtered events\u001b[0m\n",
      "\u001b[32m2025-11-03 23:55:12.077\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mafml.strategies.signal_processing\u001b[0m:\u001b[36mget_entries\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mBollinger_w20_std2 | 10,384 (14.84%) trade events selected by CUSUM filter using series.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "bb_side = bb_strategy.generate_signals(bb_time_bars)\n",
    "bb_df = bb_time_bars.loc[sample_start : sample_end]\n",
    "\n",
    "print(f\"{bb_strategy.get_strategy_name()} Signals:\")\n",
    "value_counts_data(bb_side.reindex(bb_df.index), verbose=True)\n",
    "\n",
    "# Volatility target for barriers\n",
    "vol_lookback = 100\n",
    "vol_target = get_daily_vol(bb_df.close, vol_lookback) * bb_vol_multiplier\n",
    "close = bb_df.close\n",
    "_, t_events = get_entries(bb_strategy, bb_df, filter_threshold=vol_target)\n",
    "\n",
    "vertical_barriers = add_vertical_barrier(t_events, close, **bb_time_horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ada8a2",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d09b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 516825 entries, 2018-01-02 23:20:00 to 2024-12-31 00:00:00\n",
      "Data columns (total 49 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   spread               516825 non-null  float32\n",
      " 1   vol                  516825 non-null  float32\n",
      " 2   h1_vol               516825 non-null  float32\n",
      " 3   h4_vol               516825 non-null  float32\n",
      " 4   d1_vol               516825 non-null  float32\n",
      " 5   ret                  516825 non-null  float32\n",
      " 6   ret_5                516825 non-null  float32\n",
      " 7   ret_10               516825 non-null  float32\n",
      " 8   ret_1_lag_1          516825 non-null  float32\n",
      " 9   ret_5_lag_1          516825 non-null  float32\n",
      " 10  ret_10_lag_1         516825 non-null  float32\n",
      " 11  ret_1_lag_2          516825 non-null  float32\n",
      " 12  ret_5_lag_2          516825 non-null  float32\n",
      " 13  ret_10_lag_2         516825 non-null  float32\n",
      " 14  ret_1_lag_3          516825 non-null  float32\n",
      " 15  ret_5_lag_3          516825 non-null  float32\n",
      " 16  ret_10_lag_3         516825 non-null  float32\n",
      " 17  ret_skew             516825 non-null  float32\n",
      " 18  ret_kurt             516825 non-null  float32\n",
      " 19  autocorr             516825 non-null  float32\n",
      " 20  autocorr_1           516825 non-null  float32\n",
      " 21  autocorr_2           516825 non-null  float32\n",
      " 22  autocorr_3           516825 non-null  float32\n",
      " 23  autocorr_4           516825 non-null  float32\n",
      " 24  autocorr_5           516825 non-null  float32\n",
      " 25  bbb_20_2.0           516825 non-null  float32\n",
      " 26  bbp_20_2.0           516825 non-null  float64\n",
      " 27  truerange_1          516825 non-null  float32\n",
      " 28  atrr_14              516825 non-null  float32\n",
      " 29  rsi_14               516825 non-null  float32\n",
      " 30  stochrsik_14_14_3_3  516825 non-null  float32\n",
      " 31  stochrsid_14_14_3_3  516825 non-null  float32\n",
      " 32  adx_14               516825 non-null  float32\n",
      " 33  dmp_14               516825 non-null  float32\n",
      " 34  dmn_14               516825 non-null  float32\n",
      " 35  dm_net               516825 non-null  float32\n",
      " 36  macd_12_26_9         516825 non-null  float32\n",
      " 37  macdh_12_26_9        516825 non-null  float32\n",
      " 38  sma_diff_10_20       516825 non-null  float32\n",
      " 39  sma_diff_10_100      516825 non-null  float32\n",
      " 40  sma_diff_100_200     516825 non-null  float32\n",
      " 41  sma_cross_10_20      516825 non-null  int8   \n",
      " 42  sma_cross_10_50      516825 non-null  int8   \n",
      " 43  sma_cross_10_100     516825 non-null  int8   \n",
      " 44  sma_cross_10_200     516825 non-null  int8   \n",
      " 45  sma_cross_50_100     516825 non-null  int8   \n",
      " 46  sma_cross_100_200    516825 non-null  int8   \n",
      " 47  prev_signal          516825 non-null  int8   \n",
      " 48  side                 516825 non-null  int8   \n",
      "dtypes: float32(40), float64(1), int8(8)\n",
      "memory usage: 90.7 MB\n"
     ]
    }
   ],
   "source": [
    "bb_feat = create_bollinger_features(bb_time_bars, bb_period, bb_std)\n",
    "bb_feat_time = bb_feat.join(bb_side, how=\"inner\")\n",
    "bb_feat_time.info()\n",
    "# not_stationary = is_stationary(bb_feat_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe1c961",
   "metadata": {},
   "source": [
    "#### Triple-Barrier Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaf6686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triple-Barrier (pt=1, sl=2, h={'days': 1}):\n",
      "\n",
      "     count  proportion\n",
      "bin                   \n",
      "-1   5,109    0.505741\n",
      " 1   4,993    0.494259\n",
      "\n",
      "Average Uniqueness: 0.7465\n"
     ]
    }
   ],
   "source": [
    "bb_events_tb = triple_barrier_labels(\n",
    "    close,\n",
    "    vol_target,\n",
    "    t_events,\n",
    "    pt_sl=[bb_pt_barrier, bb_sl_barrier],\n",
    "    min_ret=min_ret,\n",
    "    vertical_barrier_times=vertical_barriers,\n",
    "    vertical_barrier_zero=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "bb_events_tb_time = bb_events_tb.copy()\n",
    "print(f\"Triple-Barrier (pt={bb_pt_barrier}, sl={bb_sl_barrier}, h={bb_time_horizon}):\")\n",
    "value_counts_data(bb_events_tb['bin'], verbose=True)\n",
    "\n",
    "weights = get_event_weights(bb_events_tb, close)\n",
    "av_uniqueness = weights['tW'].mean()\n",
    "print(f\"Average Uniqueness: {av_uniqueness:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b85936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triple-Barrier (pt=1, sl=2, h={'days': 1}):\n",
      "\n",
      "     count  proportion\n",
      "bin                   \n",
      "1    6,506    0.626601\n",
      "0    3,877    0.373399\n",
      "\n",
      "Average Uniqueness: 0.5488\n"
     ]
    }
   ],
   "source": [
    "bb_events_tb = triple_barrier_labels(\n",
    "    close,\n",
    "    vol_target,\n",
    "    t_events,\n",
    "    pt_sl=[bb_pt_barrier, bb_sl_barrier],\n",
    "    min_ret=min_ret,\n",
    "    vertical_barrier_times=vertical_barriers,\n",
    "    side_prediction=bb_side,\n",
    "    vertical_barrier_zero=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "bb_events_tb_time_meta = bb_events_tb.copy()\n",
    "print(f\"Triple-Barrier (pt={bb_pt_barrier}, sl={bb_sl_barrier}, h={bb_time_horizon}):\")\n",
    "value_counts_data(bb_events_tb['bin'], verbose=True)\n",
    "\n",
    "weights = get_event_weights(bb_events_tb, close)\n",
    "av_uniqueness = weights['tW'].mean()\n",
    "print(f\"Average Uniqueness: {av_uniqueness:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac89d4",
   "metadata": {},
   "source": [
    "#### Primary Model - CV of Weighting Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83808c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import cpu_count\n",
    "\n",
    "# Reserve 1 CPU if you want to do something else during training, otherwise set to -1\n",
    "N_JOBS = cpu_count() - 1\n",
    "N_ESTIMATORS = 100\n",
    "random_state = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9509134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = bb_events_tb_time.copy()\n",
    "X = bb_feat_time.reindex(cont.index)\n",
    "y = cont[\"bin\"]\n",
    "t1 = cont[\"t1\"]\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "train, test = PurgedSplit(t1, test_size).split(X)\n",
    "X_train, X_test, y_train, y_test = (\n",
    "        X.iloc[train],\n",
    "        X.iloc[test],\n",
    "        y.iloc[train],\n",
    "        y.iloc[test],\n",
    "    )\n",
    "\n",
    "cont_train = get_event_weights(cont.iloc[train], bb_df.close)\n",
    "bb_cont_train = cont_train.copy()\n",
    "\n",
    "n_splits = 5\n",
    "pct_embargo = 0.01\n",
    "cv_gen = PurgedKFold(n_splits, cont_train.t1, pct_embargo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e03121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Uniqueness in Training Set: 0.7473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['unweighted', 'uniqueness', 'return'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_u = cont_train.tW.mean()\n",
    "print(f\"Average Uniqueness in Training Set: {avg_u:.4f}\")\n",
    "\n",
    "weighting_schemes = {\n",
    "    \"unweighted\": pd.Series(1., index=cont_train.index),\n",
    "    \"uniqueness\": cont_train[\"tW\"],\n",
    "    \"return\": cont_train[\"w\"],\n",
    "    }\n",
    "\n",
    "decay_factors = [0.0, 0.25, 0.5, 0.75]\n",
    "time_decay_weights = {}\n",
    "for time_decay in decay_factors:\n",
    "    decay_w = get_weights_by_time_decay_optimized(\n",
    "                triple_barrier_events=cont,\n",
    "                close_index=close.index,\n",
    "                last_weight=time_decay,\n",
    "                linear=True,\n",
    "                av_uniqueness=cont_train[\"tW\"],\n",
    "            )\n",
    "    time_decay_weights[f\"decay_{time_decay}\"] = decay_w\n",
    "        \n",
    "# for k, v in time_decay_weights.items():\n",
    "#     if k.startswith(\"linear\"):\n",
    "#         weighting_schemes[k] = v\n",
    "\n",
    "weighting_schemes.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beab8ff",
   "metadata": {},
   "source": [
    "##### Selection of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c2bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'standard': RandomForestClassifier(criterion='entropy', max_depth=6,\n",
       "                        min_weight_fraction_leaf=0.05, n_jobs=3, random_state=7),\n",
       " 'balanced_subsample': RandomForestClassifier(class_weight='balanced_subsample', criterion='entropy',\n",
       "                        max_depth=6, min_weight_fraction_leaf=0.05, n_jobs=3,\n",
       "                        random_state=7),\n",
       " 'max_samples': RandomForestClassifier(criterion='entropy', max_depth=6,\n",
       "                        max_samples=0.7472647467858778,\n",
       "                        min_weight_fraction_leaf=0.05, n_jobs=3, random_state=7),\n",
       " 'combined': RandomForestClassifier(class_weight='balanced_subsample', criterion='entropy',\n",
       "                        max_depth=6, max_samples=0.7472647467858778,\n",
       "                        min_weight_fraction_leaf=0.05, n_jobs=3, random_state=7)}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create multiple Random Forest configurations\n",
    "\n",
    "min_w_leaf = 0.05\n",
    "max_depth = 6\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    criterion='entropy',\n",
    "    n_estimators=N_ESTIMATORS,\n",
    "    random_state=random_state,\n",
    "    min_weight_fraction_leaf=min_w_leaf,\n",
    "    max_depth=max_depth,\n",
    "    n_jobs=N_JOBS,  # Use all available cores\n",
    "    )\n",
    "\n",
    "clf0 = rf\n",
    "clf1 = clone(rf).set_params(class_weight='balanced_subsample')\n",
    "clf2 = clone(rf).set_params(max_samples=avg_u)\n",
    "clf3 = clone(rf).set_params(max_samples=avg_u, class_weight='balanced_subsample')\n",
    "\n",
    "clfs = {k: v for k, v in zip(['standard', 'balanced_subsample', 'max_samples', 'combined'], [clf0, clf1, clf2, clf3])}\n",
    "clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06278dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier Weighting Schemes\n",
      "return standard model achieved the best neg_log_loss score of -0.6613\n",
      "\n",
      "Weighting Scheme CV:\n",
      "                          unweighted        uniqueness            return\n",
      "standard            -0.6937 ± 0.0012  -0.6934 ± 0.0013  -0.6613 ± 0.0057\n",
      "balanced_subsample  -0.6938 ± 0.0014  -0.6935 ± 0.0015  -0.6614 ± 0.0058\n",
      "max_samples         -0.6936 ± 0.0009  -0.6934 ± 0.0012  -0.6617 ± 0.0052\n",
      "combined            -0.6938 ± 0.0008  -0.6935 ± 0.0013  -0.6617 ± 0.0052\n",
      "\n",
      "Selected Best Classifier (standard): RandomForestClassifier(criterion='entropy', max_depth=6,\n",
      "                       min_weight_fraction_leaf=0.05, n_jobs=3, random_state=7)\n"
     ]
    }
   ],
   "source": [
    "# Find what model produces best CV log loss score\n",
    "\n",
    "cv_gen = PurgedKFold(n_splits, cont_train.t1, pct_embargo)\n",
    "cv_scores_d = {k: {} for k in clfs.keys()}\n",
    "print(rf.__class__.__name__, \"Weighting Schemes\")\n",
    "all_clf_scores_df = pd.DataFrame(dtype=pd.StringDtype())\n",
    "best_models = []\n",
    "best_score, best_model, best_scheme = None, None, None\n",
    "\n",
    "for scheme, sample_weights in weighting_schemes.items():\n",
    "    for param, clf in clfs.items():\n",
    "        cv_scores = ml_cross_val_score(\n",
    "            clf, X_train, y_train, cv_gen, \n",
    "            sample_weight_train=sample_weights, \n",
    "            sample_weight_score=sample_weights,\n",
    "            scoring=\"neg_log_loss\",\n",
    "        )\n",
    "        score = cv_scores.mean()\n",
    "        cv_scores_d[param][scheme] = score\n",
    "        best_score = max(best_score, score) if best_score is not None else score\n",
    "        if score == best_score:\n",
    "            best_model = param\n",
    "            best_scheme = scheme\n",
    "        all_clf_scores_df.loc[param, scheme] = f\"{cv_scores.mean():.4f} ± {cv_scores.std():.4f}\"\n",
    "\n",
    "best_clf = clone(clfs[best_model])\n",
    "print(f\"{best_scheme} {best_model} model achieved the best neg_log_loss score of {best_score:.4f}\")\n",
    "\n",
    "print(\"\\nWeighting Scheme CV:\")\n",
    "pprint(all_clf_scores_df)\n",
    "print(f\"\\nSelected Best Classifier ({best_model}): {best_clf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311350fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighting Scheme CV:\n",
      "                   accuracy              pwa      neg_log_loss  \\\n",
      "unweighted  0.5047 ± 0.0161  0.5085 ± 0.0171  -0.6937 ± 0.0012   \n",
      "uniqueness  0.5094 ± 0.0184  0.5133 ± 0.0147  -0.6934 ± 0.0013   \n",
      "return      0.6249 ± 0.0146  0.6343 ± 0.0139  -0.6613 ± 0.0057   \n",
      "\n",
      "                  precision           recall               f1  \n",
      "unweighted  0.4963 ± 0.0307  0.3294 ± 0.0785  0.3893 ± 0.0475  \n",
      "uniqueness  0.5027 ± 0.0407  0.3409 ± 0.0925  0.3972 ± 0.0626  \n",
      "return      0.6183 ± 0.0177  0.6025 ± 0.0327  0.6101 ± 0.0247  \n",
      "\n",
      "return model achieved the best neg_log_loss score of -0.6613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze all CV scores for all weighting schemes with the best model\n",
    "\n",
    "from afml.cross_validation.cross_validation import analyze_cross_val_scores\n",
    "\n",
    "all_cv_scores_d = {}\n",
    "all_cms = {}\n",
    "best_score, best_model = None, None\n",
    "all_cv_scores_df = pd.DataFrame(dtype=pd.StringDtype())\n",
    "scoring = 'f1' if set(y_train.unique()) == {0, 1} else 'neg_log_loss'\n",
    "\n",
    "for scheme, sample_weights in weighting_schemes.items():\n",
    "    cv_scores, cv_scores_df, cms = analyze_cross_val_scores(\n",
    "        best_clf, X_train, y_train, cv_gen, \n",
    "        sample_weight_train=sample_weights, \n",
    "        sample_weight_score=sample_weights,\n",
    "    )\n",
    "    score = cv_scores[scoring].mean()\n",
    "    all_cv_scores_d[scheme] = cv_scores\n",
    "    all_cms[scheme] = cms\n",
    "    best_score = max(best_score, score) if best_score is not None else score\n",
    "    if score == best_score:\n",
    "        best_scheme = scheme\n",
    "    for idx, row in cv_scores_df.iterrows():\n",
    "        all_cv_scores_df.loc[idx, scheme] = f\"{row['mean']:.4f} ± {row['std']:.4f}\"\n",
    "\n",
    "print(\"Weighting Scheme CV:\")\n",
    "pprint(all_cv_scores_df.T)\n",
    "print(f\"\\n{best_scheme} model achieved the best {scoring} score of {best_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4202d9a5",
   "metadata": {},
   "source": [
    "Test if time-decay improves performance of best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d092e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Weighting Scheme CV - Return:\n",
      "                        return  return_decay_0.0 return_decay_0.25  \\\n",
      "accuracy       0.6249 ± 0.0146   0.6236 ± 0.0117   0.6236 ± 0.0120   \n",
      "pwa            0.6343 ± 0.0139   0.6332 ± 0.0144   0.6331 ± 0.0138   \n",
      "neg_log_loss  -0.6613 ± 0.0057  -0.6628 ± 0.0057  -0.6623 ± 0.0056   \n",
      "precision      0.6183 ± 0.0177   0.6184 ± 0.0162   0.6179 ± 0.0169   \n",
      "recall         0.6025 ± 0.0327   0.5966 ± 0.0310   0.5983 ± 0.0317   \n",
      "f1             0.6101 ± 0.0247   0.6071 ± 0.0227   0.6077 ± 0.0232   \n",
      "\n",
      "              return_decay_0.5 return_decay_0.75  \n",
      "accuracy       0.6224 ± 0.0121   0.6225 ± 0.0152  \n",
      "pwa            0.6336 ± 0.0137   0.6328 ± 0.0143  \n",
      "neg_log_loss  -0.6618 ± 0.0055  -0.6621 ± 0.0058  \n",
      "precision      0.6159 ± 0.0176   0.6156 ± 0.0191  \n",
      "recall         0.5999 ± 0.0297   0.6007 ± 0.0335  \n",
      "f1             0.6076 ± 0.0225   0.6079 ± 0.0257  \n",
      "\n",
      "return model achieved the best neg_log_loss score of -0.6613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model_decay_cv_scores = all_cv_scores_df[[best_scheme]]\n",
    "\n",
    "for scheme, decay_factor in time_decay_weights.items():\n",
    "    sample_weights = weighting_schemes[best_scheme] * decay_factor\n",
    "    cv_scores, cv_scores_df, cms = analyze_cross_val_scores(\n",
    "        best_clf, X_train, y_train, cv_gen, \n",
    "        sample_weight_train=sample_weights, \n",
    "        sample_weight_score=sample_weights,\n",
    "    )\n",
    "    score = cv_scores[scoring].mean()\n",
    "    scheme = f\"{best_scheme}_{scheme}\"\n",
    "    all_cv_scores_d[scheme] = cv_scores\n",
    "    all_cms[scheme] = cms\n",
    "    best_score = max(best_score, score) if best_score is not None else score\n",
    "    for idx, row in cv_scores_df.iterrows():\n",
    "        best_model_decay_cv_scores.loc[idx, scheme] = f\"{row['mean']:.4f} ± {row['std']:.4f}\"\n",
    "    if score == best_score:\n",
    "        best_scheme = scheme\n",
    "        weighting_schemes[best_scheme] = sample_weights\n",
    "        all_cv_scores_df[scheme] = best_model_decay_cv_scores[scheme]\n",
    "        \n",
    "\n",
    "print(f\"\\nBest Weighting Scheme CV - {best_scheme.title()}:\")\n",
    "pprint(best_model_decay_cv_scores)\n",
    "\n",
    "print(f\"\\n{best_scheme} model achieved the best {scoring} score of {best_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c81fbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'fold': 1, 'TN': 597.74, 'FP': 306.17, 'FN': 329.18, 'TP': 548.78},\n",
      " {'fold': 2, 'TN': 383.48, 'FP': 200.75, 'FN': 226.25, 'TP': 305.54},\n",
      " {'fold': 3, 'TN': 536.6, 'FP': 322.27, 'FN': 310.15, 'TP': 566.3},\n",
      " {'fold': 4, 'TN': 424.97, 'FP': 234.74, 'FN': 280.48, 'TP': 352.45},\n",
      " {'fold': 5, 'TN': 722.81, 'FP': 404.05, 'FN': 401.37, 'TP': 627.92}]\n"
     ]
    }
   ],
   "source": [
    "all_cms[scheme] = cms\n",
    "# pprint(all_cms, sort_dicts=False)\n",
    "pprint(all_cms[best_scheme], sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a498b33d",
   "metadata": {},
   "source": [
    "##### Sequential Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7e9d79fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading 'afml.cross_validation.cross_validation'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "unweighted",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "uniqueness",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "return",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "seq_bootstrap_unweighted",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "seq_bootstrap_return",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "ad6c745c-7f16-43aa-bc07-4579150555cb",
       "rows": [
        [
         "accuracy",
         "0.5047 ± 0.0161",
         "0.5094 ± 0.0184",
         "0.6249 ± 0.0146",
         "0.4983 ± 0.0076",
         "0.5862 ± 0.0117"
        ],
        [
         "pwa",
         "0.5085 ± 0.0171",
         "0.5133 ± 0.0147",
         "0.6343 ± 0.0139",
         "0.5054 ± 0.0079",
         "0.6014 ± 0.0118"
        ],
        [
         "neg_log_loss",
         "-0.6937 ± 0.0012",
         "-0.6934 ± 0.0013",
         "-0.6613 ± 0.0057",
         "-0.6935 ± 0.0005",
         "-0.6823 ± 0.0025"
        ],
        [
         "precision",
         "0.4963 ± 0.0307",
         "0.5027 ± 0.0407",
         "0.6183 ± 0.0177",
         "0.4714 ± 0.0202",
         "0.5906 ± 0.0193"
        ],
        [
         "recall",
         "0.3294 ± 0.0785",
         "0.3409 ± 0.0925",
         "0.6025 ± 0.0327",
         "0.2219 ± 0.0500",
         "0.4965 ± 0.0592"
        ],
        [
         "f1",
         "0.3893 ± 0.0475",
         "0.3972 ± 0.0626",
         "0.6101 ± 0.0247",
         "0.2989 ± 0.0504",
         "0.5375 ± 0.0381"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unweighted</th>\n",
       "      <th>uniqueness</th>\n",
       "      <th>return</th>\n",
       "      <th>seq_bootstrap_unweighted</th>\n",
       "      <th>seq_bootstrap_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.5047 ± 0.0161</td>\n",
       "      <td>0.5094 ± 0.0184</td>\n",
       "      <td>0.6249 ± 0.0146</td>\n",
       "      <td>0.4983 ± 0.0076</td>\n",
       "      <td>0.5862 ± 0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pwa</th>\n",
       "      <td>0.5085 ± 0.0171</td>\n",
       "      <td>0.5133 ± 0.0147</td>\n",
       "      <td>0.6343 ± 0.0139</td>\n",
       "      <td>0.5054 ± 0.0079</td>\n",
       "      <td>0.6014 ± 0.0118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_log_loss</th>\n",
       "      <td>-0.6937 ± 0.0012</td>\n",
       "      <td>-0.6934 ± 0.0013</td>\n",
       "      <td>-0.6613 ± 0.0057</td>\n",
       "      <td>-0.6935 ± 0.0005</td>\n",
       "      <td>-0.6823 ± 0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.4963 ± 0.0307</td>\n",
       "      <td>0.5027 ± 0.0407</td>\n",
       "      <td>0.6183 ± 0.0177</td>\n",
       "      <td>0.4714 ± 0.0202</td>\n",
       "      <td>0.5906 ± 0.0193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.3294 ± 0.0785</td>\n",
       "      <td>0.3409 ± 0.0925</td>\n",
       "      <td>0.6025 ± 0.0327</td>\n",
       "      <td>0.2219 ± 0.0500</td>\n",
       "      <td>0.4965 ± 0.0592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.3893 ± 0.0475</td>\n",
       "      <td>0.3972 ± 0.0626</td>\n",
       "      <td>0.6101 ± 0.0247</td>\n",
       "      <td>0.2989 ± 0.0504</td>\n",
       "      <td>0.5375 ± 0.0381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    unweighted        uniqueness            return  \\\n",
       "accuracy       0.5047 ± 0.0161   0.5094 ± 0.0184   0.6249 ± 0.0146   \n",
       "pwa            0.5085 ± 0.0171   0.5133 ± 0.0147   0.6343 ± 0.0139   \n",
       "neg_log_loss  -0.6937 ± 0.0012  -0.6934 ± 0.0013  -0.6613 ± 0.0057   \n",
       "precision      0.4963 ± 0.0307   0.5027 ± 0.0407   0.6183 ± 0.0177   \n",
       "recall         0.3294 ± 0.0785   0.3409 ± 0.0925   0.6025 ± 0.0327   \n",
       "f1             0.3893 ± 0.0475   0.3972 ± 0.0626   0.6101 ± 0.0247   \n",
       "\n",
       "             seq_bootstrap_unweighted seq_bootstrap_return  \n",
       "accuracy              0.4983 ± 0.0076      0.5862 ± 0.0117  \n",
       "pwa                   0.5054 ± 0.0079      0.6014 ± 0.0118  \n",
       "neg_log_loss         -0.6935 ± 0.0005     -0.6823 ± 0.0025  \n",
       "precision             0.4714 ± 0.0202      0.5906 ± 0.0193  \n",
       "recall                0.2219 ± 0.0500      0.4965 ± 0.0592  \n",
       "f1                    0.2989 ± 0.0504      0.5375 ± 0.0381  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base estimator for use with sequential bootstrapping\n",
    "# I chose it beacause the default behaviour of RF is to set max_features='sqrt'\n",
    "base_rf = clone(best_clf)\n",
    "base_rf.set_params(bootstrap=False, n_estimators=1, random_state=None, n_jobs=1)\n",
    "\n",
    "seq_rf = SequentiallyBootstrappedBaggingClassifier(\n",
    "    samples_info_sets=cont_train.t1,\n",
    "    price_bars_index=bb_df.index,\n",
    "    estimator=base_rf,\n",
    "    n_estimators=20, # set low to save time\n",
    "    max_features=1,\n",
    "    max_samples=0.5,\n",
    "    bootstrap_features=True,\n",
    "    oob_score=True,\n",
    "    n_jobs=N_JOBS,\n",
    "    random_state=random_state,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "for scheme in (\"unweighted\", best_scheme):\n",
    "    w = weighting_schemes[best_scheme] if scheme is not \"unweighted\" else None \n",
    "    cv_scores, cv_scores_df, cms = analyze_cross_val_scores(\n",
    "            seq_rf, X_train, y_train, cv_gen, \n",
    "            sample_weight_train=w, \n",
    "            sample_weight_score=w,\n",
    "        )\n",
    "    scheme = f'seq_bootstrap_{scheme}'\n",
    "    all_cms[scheme] = cms\n",
    "    \n",
    "    for idx, row in cv_scores_df.iterrows():\n",
    "        all_cv_scores_df.loc[idx, scheme] = f\"{row['mean']:.4f} ± {row['std']:.4f}\"\n",
    "\n",
    "bb_all_cv_scores_df_primary = all_cv_scores_df.copy()\n",
    "all_cv_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e7d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Bootstrap done in 0 days 00:00:02\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "standard_rf",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sequential_rf",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "646c4354-3b9a-46e8-b78d-b5637b3a8f2c",
       "rows": [
        [
         "accuracy",
         "0.4812",
         "0.5134"
        ],
        [
         "pwa",
         "0.4865",
         "0.5134"
        ],
        [
         "neg_log_loss",
         "-0.7218",
         "-17.5401"
        ],
        [
         "precision",
         "0.4946",
         "0.5134"
        ],
        [
         "recall",
         "0.486",
         "1.0"
        ],
        [
         "f1",
         "0.4903",
         "0.6784"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standard_rf</th>\n",
       "      <th>sequential_rf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.4812</td>\n",
       "      <td>0.5134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pwa</th>\n",
       "      <td>0.4865</td>\n",
       "      <td>0.5134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_log_loss</th>\n",
       "      <td>-0.7218</td>\n",
       "      <td>-17.5401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.4946</td>\n",
       "      <td>0.5134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.4860</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.4903</td>\n",
       "      <td>0.6784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              standard_rf  sequential_rf\n",
       "accuracy           0.4812         0.5134\n",
       "pwa                0.4865         0.5134\n",
       "neg_log_loss      -0.7218       -17.5401\n",
       "precision          0.4946         0.5134\n",
       "recall             0.4860         1.0000\n",
       "f1                 0.4903         0.6784"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = weighting_schemes[best_scheme]\n",
    "rf = best_clf.set_params(oob_score=True).fit(\n",
    "    X_train, y_train, sample_weight=w,\n",
    ")\n",
    "\n",
    "time0 = time.time()\n",
    "seq_rf.set_params(oob_score=True, n_estimators=50).fit(\n",
    "    X_train, y_train, sample_weight=w,\n",
    ")\n",
    "seq_rfu = clone(seq_rf).fit(X_train, y_train)\n",
    "time1 = pd.Timedelta(seconds=time.time() - time0).round('1s')\n",
    "print(f\"Sequential Bootstrap done in {time1}\")\n",
    "\n",
    "ensembles = {\n",
    "    \"standard_rf\": {\"classifier\": rf, \n",
    "                    \"pred\": rf.predict(X_test),\n",
    "                    \"prob\": rf.predict_proba(X_test),\n",
    "                    \"oob\": rf.oob_score_,\n",
    "                },\n",
    "    \"sequential_rf\": {\"classifier\": seq_rf, \n",
    "                      \"pred\": seq_rf.predict(X_test),\n",
    "                      \"prob\": seq_rf.predict_proba(X_test),\n",
    "                      \"oob\": seq_rf.oob_score_,\n",
    "                      },\n",
    "    \"sequential_rf_unweighted\": {\"classifier\": seq_rfu, \n",
    "                                 \"pred\": seq_rfu.predict(X_test),\n",
    "                                 \"prob\": seq_rfu.predict_proba(X_test),\n",
    "                                 \"oob\": seq_rfu.oob_score_,\n",
    "                                 },\n",
    "}\n",
    "\n",
    "scoring_methods = {\n",
    "            \"accuracy\": accuracy_score,\n",
    "            \"pwa\": probability_weighted_accuracy,\n",
    "            \"neg_log_loss\": log_loss,\n",
    "            \"precision\": precision_score,\n",
    "            \"recall\": recall_score,\n",
    "            \"f1\": f1_score,\n",
    "        }\n",
    "\n",
    "all_scores_oos = pd.DataFrame()\n",
    "\n",
    "for clf in ensembles.keys():\n",
    "    for method, scoring in scoring_methods.items():\n",
    "        if scoring in (probability_weighted_accuracy, log_loss):\n",
    "            y_pred = ensembles[clf][\"prob\"]\n",
    "        else:\n",
    "            y_pred = ensembles[clf][\"pred\"]\n",
    "        score = scoring(y_test, y_pred)\n",
    "        if method == \"neg_log_loss\":\n",
    "            score *= -1\n",
    "        all_scores_oos.loc[method, clf] = score\n",
    "\n",
    "bb_all_scores_oos_primary = all_scores_oos.copy()\n",
    "all_scores_oos.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042bf386",
   "metadata": {},
   "outputs": [],
   "source": [
    "winsound.Beep(1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e170c",
   "metadata": {},
   "source": [
    "#### Meta-Labelled CV of Weighting Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import cpu_count\n",
    "\n",
    "# Reserve 1 CPU if you want to do something else during training, otherwise set to -1\n",
    "N_JOBS = cpu_count() - 1\n",
    "N_ESTIMATORS = 100\n",
    "random_state = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85333ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = bb_events_tb_time_meta.copy()\n",
    "X = bb_feat_time.reindex(cont.index)\n",
    "y = cont[\"bin\"]\n",
    "t1 = cont[\"t1\"]\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "train, test = PurgedSplit(t1, test_size).split(X)\n",
    "X_train, X_test, y_train, y_test = (\n",
    "        X.iloc[train],\n",
    "        X.iloc[test],\n",
    "        y.iloc[train],\n",
    "        y.iloc[test],\n",
    "    )\n",
    "\n",
    "cont_train = get_event_weights(cont.iloc[train], bb_df.close)\n",
    "bb_cont_train = cont_train.copy()\n",
    "\n",
    "n_splits = 5\n",
    "pct_embargo = 0.01\n",
    "cv_gen = PurgedKFold(n_splits, cont_train.t1, pct_embargo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f71a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Uniqueness in Training Set: 0.5473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['unweighted', 'uniqueness', 'return'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_u = cont_train.tW.mean()\n",
    "print(f\"Average Uniqueness in Training Set: {avg_u:.4f}\")\n",
    "\n",
    "weighting_schemes = {\n",
    "    \"unweighted\": pd.Series(1., index=cont_train.index),\n",
    "    \"uniqueness\": cont_train[\"tW\"],\n",
    "    \"return\": cont_train[\"w\"],\n",
    "    }\n",
    "\n",
    "decay_factors = [0.0, 0.25, 0.5, 0.75]\n",
    "time_decay_weights = {}\n",
    "for time_decay in decay_factors:\n",
    "    decay_w = get_weights_by_time_decay_optimized(\n",
    "                triple_barrier_events=cont,\n",
    "                close_index=close.index,\n",
    "                last_weight=time_decay,\n",
    "                linear=True,\n",
    "                av_uniqueness=cont_train[\"tW\"],\n",
    "            )\n",
    "    time_decay_weights[f\"decay_{time_decay}\"] = decay_w\n",
    "        \n",
    "# for k, v in time_decay_weights.items():\n",
    "#     if k.startswith(\"linear\"):\n",
    "#         weighting_schemes[k] = v\n",
    "\n",
    "weighting_schemes.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421bbf0d",
   "metadata": {},
   "source": [
    "##### Selection of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee3175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'standard': RandomForestClassifier(criterion='entropy', max_depth=6,\n",
       "                        min_weight_fraction_leaf=0.05, n_jobs=3, random_state=7),\n",
       " 'balanced_subsample': RandomForestClassifier(class_weight='balanced_subsample', criterion='entropy',\n",
       "                        max_depth=6, min_weight_fraction_leaf=0.05, n_jobs=3,\n",
       "                        random_state=7),\n",
       " 'max_samples': RandomForestClassifier(criterion='entropy', max_depth=6,\n",
       "                        max_samples=0.5473256253464687,\n",
       "                        min_weight_fraction_leaf=0.05, n_jobs=3, random_state=7),\n",
       " 'combined': RandomForestClassifier(class_weight='balanced_subsample', criterion='entropy',\n",
       "                        max_depth=6, max_samples=0.5473256253464687,\n",
       "                        min_weight_fraction_leaf=0.05, n_jobs=3, random_state=7)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create multiple Random Forest configurations\n",
    "\n",
    "min_w_leaf = 0.05\n",
    "max_depth = 6\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    criterion='entropy',\n",
    "    n_estimators=N_ESTIMATORS,\n",
    "    random_state=random_state,\n",
    "    min_weight_fraction_leaf=min_w_leaf,\n",
    "    max_depth=max_depth,\n",
    "    n_jobs=N_JOBS,  # Use all available cores\n",
    "    )\n",
    "\n",
    "clf0 = rf\n",
    "clf1 = clone(rf).set_params(class_weight='balanced_subsample')\n",
    "clf2 = clone(rf).set_params(max_samples=avg_u)\n",
    "clf3 = clone(rf).set_params(max_samples=avg_u, class_weight='balanced_subsample')\n",
    "\n",
    "clfs = {k: v for k, v in zip(['standard', 'balanced_subsample', 'max_samples', 'combined'], [clf0, clf1, clf2, clf3])}\n",
    "clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c77527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier Weighting Schemes\n",
      "return max_samples model achieved the best neg_log_loss score of -0.6597\n",
      "\n",
      "Weighting Scheme CV:\n",
      "                          unweighted        uniqueness            return\n",
      "standard            -0.6611 ± 0.0095  -0.6647 ± 0.0099  -0.6600 ± 0.0023\n",
      "balanced_subsample  -0.6919 ± 0.0034  -0.6956 ± 0.0045  -0.6847 ± 0.0036\n",
      "max_samples         -0.6612 ± 0.0101  -0.6646 ± 0.0099  -0.6597 ± 0.0023\n",
      "combined            -0.6919 ± 0.0029  -0.6954 ± 0.0048  -0.6831 ± 0.0042\n",
      "\n",
      "Selected Best Classifier (max_samples): RandomForestClassifier(criterion='entropy', max_depth=6,\n",
      "                       max_samples=0.5473256253464687,\n",
      "                       min_weight_fraction_leaf=0.05, n_jobs=3, random_state=7)\n"
     ]
    }
   ],
   "source": [
    "# Find what model produces best CV log loss score\n",
    "\n",
    "cv_gen = PurgedKFold(n_splits, cont_train.t1, pct_embargo)\n",
    "cv_scores_d = {k: {} for k in clfs.keys()}\n",
    "print(rf.__class__.__name__, \"Weighting Schemes\")\n",
    "all_clf_scores_df = pd.DataFrame(dtype=pd.StringDtype())\n",
    "best_models = []\n",
    "best_score, best_model, best_scheme = None, None, None\n",
    "\n",
    "for scheme, sample_weights in weighting_schemes.items():\n",
    "    for param, clf in clfs.items():\n",
    "        cv_scores = ml_cross_val_score(\n",
    "            clf, X_train, y_train, cv_gen, \n",
    "            sample_weight_train=sample_weights, \n",
    "            sample_weight_score=sample_weights,\n",
    "            scoring=\"neg_log_loss\",\n",
    "        )\n",
    "        score = cv_scores.mean()\n",
    "        cv_scores_d[param][scheme] = score\n",
    "        best_score = max(best_score, score) if best_score is not None else score\n",
    "        if score == best_score:\n",
    "            best_model = param\n",
    "            best_scheme = scheme\n",
    "        all_clf_scores_df.loc[param, scheme] = f\"{cv_scores.mean():.4f} ± {cv_scores.std():.4f}\"\n",
    "\n",
    "best_clf = clone(clfs[best_model])\n",
    "print(f\"{best_scheme} {best_model} model achieved the best neg_log_loss score of {best_score:.4f}\")\n",
    "\n",
    "print(\"\\nWeighting Scheme CV:\")\n",
    "pprint(all_clf_scores_df)\n",
    "print(f\"\\nSelected Best Classifier ({best_model}): {best_clf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1663541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighting Scheme CV:\n",
      "                   accuracy              pwa      neg_log_loss  \\\n",
      "unweighted  0.6262 ± 0.0159  0.6310 ± 0.0181  -0.6612 ± 0.0101   \n",
      "uniqueness  0.6172 ± 0.0166  0.6258 ± 0.0185  -0.6646 ± 0.0099   \n",
      "return      0.6245 ± 0.0048  0.6347 ± 0.0052  -0.6597 ± 0.0023   \n",
      "\n",
      "                  precision           recall               f1  \n",
      "unweighted  0.6262 ± 0.0159  1.0000 ± 0.0000  0.7700 ± 0.0119  \n",
      "uniqueness  0.6175 ± 0.0164  0.9989 ± 0.0013  0.7631 ± 0.0126  \n",
      "return      0.1482 ± 0.2964  0.0013 ± 0.0027  0.0026 ± 0.0053  \n",
      "\n",
      "unweighted model achieved the best f1 score of 0.7700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze all CV scores for all weighting schemes with the best model\n",
    "\n",
    "from afml.cross_validation.cross_validation import analyze_cross_val_scores\n",
    "\n",
    "all_cv_scores_d = {}\n",
    "all_cms = {}\n",
    "best_score, best_model = None, None\n",
    "all_cv_scores_df = pd.DataFrame(dtype=pd.StringDtype())\n",
    "scoring = 'f1' if set(y_train.unique()) == {0, 1} else 'neg_log_loss'\n",
    "\n",
    "for scheme, sample_weights in weighting_schemes.items():\n",
    "    cv_scores, cv_scores_df, cms = analyze_cross_val_scores(\n",
    "        best_clf, X_train, y_train, cv_gen, \n",
    "        sample_weight_train=sample_weights, \n",
    "        sample_weight_score=sample_weights,\n",
    "    )\n",
    "    score = cv_scores[scoring].mean()\n",
    "    all_cv_scores_d[scheme] = cv_scores\n",
    "    all_cms[scheme] = cms\n",
    "    best_score = max(best_score, score) if best_score is not None else score\n",
    "    if score == best_score:\n",
    "        best_scheme = scheme\n",
    "    for idx, row in cv_scores_df.iterrows():\n",
    "        all_cv_scores_df.loc[idx, scheme] = f\"{row['mean']:.4f} ± {row['std']:.4f}\"\n",
    "\n",
    "print(\"Weighting Scheme CV:\")\n",
    "pprint(all_cv_scores_df.T)\n",
    "print(f\"\\n{best_scheme} model achieved the best {scoring} score of {best_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea4488",
   "metadata": {},
   "source": [
    "Test if time-decay improves performance of best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a14d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Weighting Scheme CV - Unweighted:\n",
      "                    unweighted unweighted_decay_0.0 unweighted_decay_0.25  \\\n",
      "accuracy       0.6262 ± 0.0159      0.6253 ± 0.0152       0.6258 ± 0.0155   \n",
      "pwa            0.6310 ± 0.0181      0.6283 ± 0.0153       0.6300 ± 0.0163   \n",
      "neg_log_loss  -0.6612 ± 0.0101     -0.6622 ± 0.0086      -0.6614 ± 0.0091   \n",
      "precision      0.6262 ± 0.0159      0.6253 ± 0.0152       0.6258 ± 0.0155   \n",
      "recall         1.0000 ± 0.0000      1.0000 ± 0.0000       1.0000 ± 0.0000   \n",
      "f1             0.7700 ± 0.0119      0.7693 ± 0.0114       0.7697 ± 0.0116   \n",
      "\n",
      "             unweighted_decay_0.5 unweighted_decay_0.75  \n",
      "accuracy          0.6260 ± 0.0157       0.6261 ± 0.0158  \n",
      "pwa               0.6303 ± 0.0172       0.6306 ± 0.0177  \n",
      "neg_log_loss     -0.6613 ± 0.0096      -0.6613 ± 0.0099  \n",
      "precision         0.6260 ± 0.0157       0.6261 ± 0.0158  \n",
      "recall            1.0000 ± 0.0000       1.0000 ± 0.0000  \n",
      "f1                0.7699 ± 0.0117       0.7699 ± 0.0118  \n",
      "\n",
      "unweighted model achieved the best f1 score of 0.7700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model_decay_cv_scores = all_cv_scores_df[[best_scheme]]\n",
    "\n",
    "for scheme, decay_factor in time_decay_weights.items():\n",
    "    sample_weights = weighting_schemes[best_scheme] * decay_factor\n",
    "    cv_scores, cv_scores_df, cms = analyze_cross_val_scores(\n",
    "        best_clf, X_train, y_train, cv_gen, \n",
    "        sample_weight_train=sample_weights, \n",
    "        sample_weight_score=sample_weights,\n",
    "    )\n",
    "    score = cv_scores[scoring].mean()\n",
    "    scheme = f\"{best_scheme}_{scheme}\"\n",
    "    all_cv_scores_d[scheme] = cv_scores\n",
    "    all_cms[scheme] = cms\n",
    "    best_score = max(best_score, score) if best_score is not None else score\n",
    "    for idx, row in cv_scores_df.iterrows():\n",
    "        best_model_decay_cv_scores.loc[idx, scheme] = f\"{row['mean']:.4f} ± {row['std']:.4f}\"\n",
    "    if score == best_score:\n",
    "        best_scheme = scheme\n",
    "        weighting_schemes[best_scheme] = sample_weights\n",
    "        all_cv_scores_df[scheme] = best_model_decay_cv_scores[scheme]\n",
    "        \n",
    "\n",
    "print(f\"\\nBest Weighting Scheme CV - {best_scheme.title()}:\")\n",
    "pprint(best_model_decay_cv_scores)\n",
    "\n",
    "print(f\"\\n{best_scheme} model achieved the best {scoring} score of {best_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8cfdaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m all_cms[scheme] \u001b[38;5;241m=\u001b[39m cms\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# pprint(all_cms, sort_dicts=False)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m pprint(\u001b[43mall_cms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m]\u001b[49m, sort_dicts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyError\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "all_cms[scheme] = cms\n",
    "# pprint(all_cms, sort_dicts=False)\n",
    "pprint(all_cms[best_scheme], sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5032e5",
   "metadata": {},
   "source": [
    "##### Sequential Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b95e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequentiallyBootstrappedBaggingClassifier(estimator=RandomForestClassifier(bootstrap=False,\n",
      "                                                                           criterion='entropy',\n",
      "                                                                           max_depth=6,\n",
      "                                                                           min_weight_fraction_leaf=0.05,\n",
      "                                                                           n_estimators=1,\n",
      "                                                                           n_jobs=3),\n",
      "                                          max_features=1, n_estimators=100,\n",
      "                                          n_jobs=3, oob_score=True,\n",
      "                                          price_bars_index=DatetimeIndex(['2018-01-01 23:05:00', '2018-01-01 23:10:00',\n",
      "               '2018-01-01 23:15:00', '2018-01-01 23:20:00',...\n",
      "2018-01-03 01:30:00   2018-01-03 01:50:00\n",
      "2018-01-03 02:40:00   2018-01-03 04:00:00\n",
      "2018-01-03 05:35:00   2018-01-03 08:10:00\n",
      "2018-01-03 06:45:00   2018-01-03 07:55:00\n",
      "                              ...        \n",
      "2022-10-04 14:50:00   2022-10-04 17:30:00\n",
      "2022-10-05 02:10:00   2022-10-05 04:45:00\n",
      "2022-10-05 02:40:00   2022-10-05 09:20:00\n",
      "2022-10-05 14:50:00   2022-10-05 16:20:00\n",
      "2022-10-05 16:05:00   2022-10-06 01:55:00\n",
      "Name: t1, Length: 8082, dtype: datetime64[ns],\n",
      "                                          verbose=False)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[51], line 19\u001b[0m\n",
      "\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(seq_rf)\n",
      "\u001b[0;32m     18\u001b[0m w \u001b[38;5;241m=\u001b[39m weighting_schemes[best_scheme]\n",
      "\u001b[1;32m---> 19\u001b[0m cv_scores, cv_scores_df, cms \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_cross_val_scores\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_rf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     24\u001b[0m all_cms[scheme] \u001b[38;5;241m=\u001b[39m cms\n",
      "\u001b[0;32m     26\u001b[0m scheme \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_bootstrap\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\n",
      "File \u001b[1;32mc:\\users\\joen\\documents\\github\\machine-learning-blueprint\\afml\\cross_validation\\cross_validation.py:391\u001b[0m, in \u001b[0;36manalyze_cross_val_scores\u001b[1;34m(classifier, X, y, cv_gen, sample_weight_train, sample_weight_score)\u001b[0m\n",
      "\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seq_bootstrap:\n",
      "\u001b[0;32m    388\u001b[0m     classifier \u001b[38;5;241m=\u001b[39m clone(classifier)\u001b[38;5;241m.\u001b[39mset_params(\n",
      "\u001b[0;32m    389\u001b[0m         samples_info_sets\u001b[38;5;241m=\u001b[39mt1\u001b[38;5;241m.\u001b[39miloc[train]\n",
      "\u001b[0;32m    390\u001b[0m     )  \u001b[38;5;66;03m# Create new instance\u001b[39;00m\n",
      "\u001b[1;32m--> 391\u001b[0m fit \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    395\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    396\u001b[0m prob \u001b[38;5;241m=\u001b[39m fit\u001b[38;5;241m.\u001b[39mpredict_proba(X\u001b[38;5;241m.\u001b[39miloc[test, :])\n",
      "\u001b[0;32m    397\u001b[0m pred \u001b[38;5;241m=\u001b[39m fit\u001b[38;5;241m.\u001b[39mpredict(X\u001b[38;5;241m.\u001b[39miloc[test, :])\n",
      "\n",
      "File \u001b[1;32mc:\\users\\joen\\documents\\github\\machine-learning-blueprint\\afml\\ensemble\\sb_bagging.py:323\u001b[0m, in \u001b[0;36mSequentiallyBootstrappedBaseBagging.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n",
      "\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[0;32m    303\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m    Build a Sequentially Bootstrapped Bagging ensemble of estimators from the training\u001b[39;00m\n",
      "\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    set (X, y).\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    self : (object)\u001b[39;00m\n",
      "\u001b[0;32m    322\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m--> 323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\users\\joen\\documents\\github\\machine-learning-blueprint\\afml\\ensemble\\sb_bagging.py:570\u001b[0m, in \u001b[0;36mSequentiallyBootstrappedBaggingClassifier._fit\u001b[1;34m(self, X, y, max_samples, sample_weight)\u001b[0m\n",
      "\u001b[0;32m    567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# Call parent _fit method\u001b[39;00m\n",
      "\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\users\\joen\\documents\\github\\machine-learning-blueprint\\afml\\ensemble\\sb_bagging.py:433\u001b[0m, in \u001b[0;36mSequentiallyBootstrappedBaseBagging._fit\u001b[1;34m(self, X, y, max_samples, sample_weight)\u001b[0m\n",
      "\u001b[0;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds, seeds])\n",
      "\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# Build estimators in parallel\u001b[39;00m\n",
      "\u001b[1;32m--> 433\u001b[0m all_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_estimators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_indices_\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_n_estimators\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    446\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# Unpack results\u001b[39;00m\n",
      "\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m all_results:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\JoeN\\miniforge3\\envs\\mlfinlab_env\\lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n",
      "\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n",
      "\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n",
      "\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n",
      "\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n",
      "\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n",
      "\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\JoeN\\miniforge3\\envs\\mlfinlab_env\\lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n",
      "\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n",
      "\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n",
      "\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n",
      "\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n",
      "\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n",
      "\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n",
      "\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\JoeN\\miniforge3\\envs\\mlfinlab_env\\lib\\site-packages\\joblib\\parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n",
      "\u001b[0;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n",
      "\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n",
      "\u001b[0;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n",
      "\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n",
      "\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n",
      "\u001b[0;32m   1799\u001b[0m     ):\n",
      "\u001b[1;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n",
      "\u001b[0;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n",
      "\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Base estimator for use with sequential bootstrapping\n",
    "# I chose it beacause the default behaviour of RF is to set max_features='sqrt'\n",
    "base_rf = clone(best_clf).set_params(bootstrap=False, n_estimators=1, random_state=None, n_jobs=1)\n",
    "\n",
    "seq_rf = SequentiallyBootstrappedBaggingClassifier(\n",
    "    samples_info_sets=cont_train.t1,\n",
    "    price_bars_index=bb_df.index,\n",
    "    estimator=base_rf,\n",
    "    n_estimators=20, # set low to save time\n",
    "    max_features=1,\n",
    "    max_samples=1,\n",
    "    bootstrap_features=True,\n",
    "    oob_score=True,\n",
    "    n_jobs=N_JOBS,\n",
    "    random_state=random_state,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "w = weighting_schemes[best_scheme]\n",
    "cv_scores, cv_scores_df, cms = analyze_cross_val_scores(\n",
    "        seq_rf, X_train, y_train, cv_gen, \n",
    "        sample_weight_train=w, \n",
    "        sample_weight_score=w,\n",
    "    )\n",
    "all_cms[scheme] = cms\n",
    "\n",
    "scheme = 'seq_bootstrap'\n",
    "for idx, row in cv_scores_df.iterrows():\n",
    "    all_cv_scores_df.loc[idx, scheme] = f\"{row['mean']:.4f} ± {row['std']:.4f}\"\n",
    "\n",
    "bb_all_cv_scores_df_meta = all_cv_scores_df.copy()\n",
    "all_cv_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a13c1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Bootstrap done in 0 days 00:12:06\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "standard_rf",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sequential_rf",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sequential_rf_avgu",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sequential_rf_unweighted_avgu",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "7ed9ac37-e0c2-4db8-9da8-b21aea2df33a",
       "rows": [
        [
         "accuracy",
         "0.4855",
         "0.504",
         "0.5013",
         "0.5"
        ],
        [
         "pwa",
         "0.4918",
         "0.5024",
         "0.4899",
         "0.4916"
        ],
        [
         "neg_log_loss",
         "-0.7232",
         "-0.7016",
         "-0.7036",
         "-0.7033"
        ],
        [
         "precision",
         "0.4888",
         "0.5077",
         "0.5054",
         "0.5038"
        ],
        [
         "recall",
         "0.4852",
         "0.4728",
         "0.4315",
         "0.4315"
        ],
        [
         "f1",
         "0.487",
         "0.4896",
         "0.4655",
         "0.4649"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standard_rf</th>\n",
       "      <th>sequential_rf</th>\n",
       "      <th>sequential_rf_avgu</th>\n",
       "      <th>sequential_rf_unweighted_avgu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.4855</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.5013</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pwa</th>\n",
       "      <td>0.4918</td>\n",
       "      <td>0.5024</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.4916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_log_loss</th>\n",
       "      <td>-0.7232</td>\n",
       "      <td>-0.7016</td>\n",
       "      <td>-0.7036</td>\n",
       "      <td>-0.7033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.4888</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.5054</td>\n",
       "      <td>0.5038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.4852</td>\n",
       "      <td>0.4728</td>\n",
       "      <td>0.4315</td>\n",
       "      <td>0.4315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.4870</td>\n",
       "      <td>0.4896</td>\n",
       "      <td>0.4655</td>\n",
       "      <td>0.4649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              standard_rf  sequential_rf  sequential_rf_avgu  \\\n",
       "accuracy           0.4855         0.5040              0.5013   \n",
       "pwa                0.4918         0.5024              0.4899   \n",
       "neg_log_loss      -0.7232        -0.7016             -0.7036   \n",
       "precision          0.4888         0.5077              0.5054   \n",
       "recall             0.4852         0.4728              0.4315   \n",
       "f1                 0.4870         0.4896              0.4655   \n",
       "\n",
       "              sequential_rf_unweighted_avgu  \n",
       "accuracy                             0.5000  \n",
       "pwa                                  0.4916  \n",
       "neg_log_loss                        -0.7033  \n",
       "precision                            0.5038  \n",
       "recall                               0.4315  \n",
       "f1                                   0.4649  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = weighting_schemes[best_scheme]\n",
    "rf = best_clf.set_params(oob_score=True).fit(\n",
    "    X_train, y_train, sample_weight=w,\n",
    ")\n",
    "\n",
    "time0 = time.time()\n",
    "seq_rf.set_params(oob_score=True).fit(\n",
    "    X_train, y_train, sample_weight=w,\n",
    ")\n",
    "time1 = pd.Timedelta(seconds=time.time() - time0).round('1s')\n",
    "print(f\"Sequential Bootstrap done in {time1}\")\n",
    "\n",
    "ensembles = {\n",
    "    \"standard_rf\": {\"classifier\": rf, \n",
    "                    \"pred\": rf.predict(X_test),\n",
    "                    \"prob\": rf.predict_proba(X_test),\n",
    "                    \"oob\": rf.oob_score_,\n",
    "                },\n",
    "    \"sequential_rf\": {\"classifier\": seq_rf, \n",
    "                      \"pred\": seq_rf.predict(X_test),\n",
    "                      \"prob\": seq_rf.predict_proba(X_test),\n",
    "                      \"oob\": seq_rf.oob_score_,\n",
    "                      },\n",
    "}\n",
    "\n",
    "scoring_methods = {\n",
    "            \"accuracy\": accuracy_score,\n",
    "            \"pwa\": probability_weighted_accuracy,\n",
    "            \"neg_log_loss\": log_loss,\n",
    "            \"precision\": precision_score,\n",
    "            \"recall\": recall_score,\n",
    "            \"f1\": f1_score,\n",
    "        }\n",
    "\n",
    "all_scores_oos = pd.DataFrame()\n",
    "\n",
    "for clf in ensembles.keys():\n",
    "    for method, scoring in scoring_methods.items():\n",
    "        if scoring in (probability_weighted_accuracy, log_loss):\n",
    "            y_pred = ensembles[clf][\"prob\"]\n",
    "        else:\n",
    "            y_pred = ensembles[clf][\"pred\"]\n",
    "        score = scoring(y_test, y_pred)\n",
    "        if method == \"neg_log_loss\":\n",
    "            score *= -1\n",
    "        all_scores_oos.loc[method, clf] = score\n",
    "\n",
    "bb_all_scores_oos_meta = all_scores_oos.copy()\n",
    "all_scores_oos.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b942a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "winsound.Beep(1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec20ef",
   "metadata": {},
   "source": [
    "## 3. Moving Average Crossover Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f163b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from afml.strategies.ma_crossover_feature_engine import ForexFeatureEngine\n",
    "\n",
    "ma_timeframe = \"M5\"\n",
    "file = Path(r\"..\\data\\EURUSD_M15_time_2018-01-01-2024-12-31.parq\")\n",
    "ma_time_bars = pd.read_parquet(file)\n",
    "\n",
    "fast_window, slow_window = 20, 50\n",
    "ma_strategy = MACrossoverStrategy(fast_window, slow_window)\n",
    "ma_pt_barrier, ma_sl_barrier, ma_time_horizon = (0, 2, dict(days=5))\n",
    "ma_vol_multiplier = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e673d8",
   "metadata": {},
   "source": [
    "### Time-Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c09a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-27 06:58:41.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mafml.filters.filters\u001b[0m:\u001b[36mcusum_filter\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1m12,748 CUSUM-filtered events\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACrossover_20_50 Signals:\n",
      "\n",
      "       count  proportion\n",
      "side                    \n",
      "-1    61,845    0.502062\n",
      " 1    61,287    0.497532\n",
      " 0        50    0.000406\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-27 06:58:42.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mafml.strategies.signal_processing\u001b[0m:\u001b[36mget_entries\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mMACrossover_20_50 | 12,744 (10.35%) trade events selected by CUSUM filter (threshold = 0.1252%).\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ma_side = ma_strategy.generate_signals(ma_time_bars)\n",
    "ma_df = ma_time_bars.loc[sample_start : sample_end]\n",
    "\n",
    "\n",
    "print(f\"{ma_strategy.get_strategy_name()} Signals:\")\n",
    "value_counts_data(ma_side.reindex(ma_df.index), verbose=True)\n",
    "\n",
    "# Volatility target for barriers\n",
    "vol_lookback = fast_window\n",
    "vol_target = get_daily_vol(ma_df.close, vol_lookback) * ma_vol_multiplier\n",
    "close = ma_df.close\n",
    "\n",
    "thres = vol_target.mean()\n",
    "_, t_events = get_entries(ma_strategy, ma_df, filter_threshold=vol_target.mean())\n",
    "\n",
    "vertical_barriers = add_vertical_barrier(t_events, close, **ma_time_horizon)\n",
    "linear_decay = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f7545",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9804e688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage reduced from 106.62 MB to 55.49 MB (48.0% reduction)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 172386 entries, 2018-01-01 23:15:00 to 2024-12-31 00:00:00\n",
      "Data columns (total 94 columns):\n",
      " #   Column                          Non-Null Count   Dtype  \n",
      "---  ------                          --------------   -----  \n",
      " 0   ma_10                           172386 non-null  float32\n",
      " 1   ma_20                           172386 non-null  float32\n",
      " 2   ma_50                           172386 non-null  float32\n",
      " 3   ma_100                          172386 non-null  float32\n",
      " 4   ma_200                          172386 non-null  float32\n",
      " 5   ma_10_20_cross                  172386 non-null  float64\n",
      " 6   ma_20_50_cross                  172386 non-null  float64\n",
      " 7   ma_50_200_cross                 172386 non-null  float64\n",
      " 8   ma_spread_10_20                 172386 non-null  float32\n",
      " 9   ma_spread_20_50                 172386 non-null  float32\n",
      " 10  ma_spread_50_200                172386 non-null  float32\n",
      " 11  ma_20_slope                     172386 non-null  float32\n",
      " 12  ma_50_slope                     172386 non-null  float32\n",
      " 13  price_above_ma_20               172386 non-null  float64\n",
      " 14  price_above_ma_50               172386 non-null  float64\n",
      " 15  ma_ribbon_aligned               172386 non-null  float64\n",
      " 16  atr_14                          172386 non-null  float32\n",
      " 17  atr_21                          172386 non-null  float32\n",
      " 18  atr_regime                      172386 non-null  float32\n",
      " 19  realized_vol_10                 172386 non-null  float32\n",
      " 20  realized_vol_20                 172386 non-null  float32\n",
      " 21  realized_vol_50                 172386 non-null  float32\n",
      " 22  vol_of_vol                      172386 non-null  float32\n",
      " 23  hl_range                        172386 non-null  float32\n",
      " 24  hl_range_ma                     172386 non-null  float32\n",
      " 25  hl_range_regime                 172386 non-null  float32\n",
      " 26  bb_upper                        172386 non-null  float32\n",
      " 27  bb_lower                        172386 non-null  float32\n",
      " 28  bb_percent                      172386 non-null  float64\n",
      " 29  bb_bandwidth                    172386 non-null  float32\n",
      " 30  bb_squeeze                      172386 non-null  float32\n",
      " 31  efficiency_ratio_14             172386 non-null  float32\n",
      " 32  efficiency_ratio_30             172386 non-null  float32\n",
      " 33  adx_14                          172386 non-null  float32\n",
      " 34  dmp_14                          172386 non-null  float32\n",
      " 35  dmn_14                          172386 non-null  float32\n",
      " 36  adx_trend_strength              172386 non-null  float64\n",
      " 37  adx_trend_direction             172386 non-null  float64\n",
      " 38  trend_window                    172386 non-null  float32\n",
      " 39  trend_slope                     172386 non-null  float32\n",
      " 40  trend_t_value                   172386 non-null  float32\n",
      " 41  trend_rsquared                  172386 non-null  float32\n",
      " 42  trend_ret                       172386 non-null  float32\n",
      " 43  roc_10                          172386 non-null  float32\n",
      " 44  roc_20                          172386 non-null  float32\n",
      " 45  momentum_14                     172386 non-null  float32\n",
      " 46  hh_ll_20                        172386 non-null  float32\n",
      " 47  trend_persistence               172386 non-null  float32\n",
      " 48  return_skew_20                  172386 non-null  float32\n",
      " 49  return_kurtosis_20              172386 non-null  float32\n",
      " 50  var_95                          172386 non-null  float32\n",
      " 51  cvar_95                         172386 non-null  float32\n",
      " 52  market_stress                   172386 non-null  float64\n",
      " 53  current_drawdown                172386 non-null  float32\n",
      " 54  days_since_high                 172386 non-null  float64\n",
      " 55  hour_sin_h1                     172386 non-null  float32\n",
      " 56  hour_cos_h1                     172386 non-null  float32\n",
      " 57  hour_sin_h2                     172386 non-null  float32\n",
      " 58  hour_cos_h2                     172386 non-null  float32\n",
      " 59  hour_sin_h3                     172386 non-null  float32\n",
      " 60  hour_cos_h3                     172386 non-null  float32\n",
      " 61  dayofweek_sin                   172386 non-null  float32\n",
      " 62  dayofweek_cos                   172386 non-null  float32\n",
      " 63  dayofyear_sin                   172386 non-null  float32\n",
      " 64  dayofyear_cos                   172386 non-null  float32\n",
      " 65  sydney_session                  172386 non-null  float64\n",
      " 66  tokyo_session                   172386 non-null  float64\n",
      " 67  london_session                  172386 non-null  float64\n",
      " 68  ny_session                      172386 non-null  float64\n",
      " 69  session_overlap                 172386 non-null  float64\n",
      " 70  friday_ny_close                 172386 non-null  float64\n",
      " 71  sunday_open                     172386 non-null  float64\n",
      " 72  month_end                       172386 non-null  float64\n",
      " 73  quarter_end                     172386 non-null  float64\n",
      " 74  sydney_session_vol              172386 non-null  float32\n",
      " 75  tokyo_session_vol               172386 non-null  float32\n",
      " 76  london_session_vol              172386 non-null  float32\n",
      " 77  ny_session_vol                  172386 non-null  float32\n",
      " 78  session_overlap_vol             172386 non-null  float32\n",
      " 79  friday_ny_close_vol             172386 non-null  float32\n",
      " 80  month_end_vol                   172386 non-null  float32\n",
      " 81  quarter_end_vol                 172386 non-null  float32\n",
      " 82  doji                            172386 non-null  float64\n",
      " 83  hammer                          172386 non-null  float64\n",
      " 84  inside_bar                      172386 non-null  float64\n",
      " 85  outside_bar                     172386 non-null  float64\n",
      " 86  near_recent_high                172386 non-null  float64\n",
      " 87  near_recent_low                 172386 non-null  float64\n",
      " 88  fractal_trend_strength          172386 non-null  float32\n",
      " 89  fractal_trend_direction         172386 non-null  float32\n",
      " 90  fractal_ma_ratio                172386 non-null  float32\n",
      " 91  fractal_trend_confirmation      172386 non-null  float64\n",
      " 92  distance_to_fractal_resistance  172386 non-null  float32\n",
      " 93  distance_to_fractal_support     172386 non-null  float32\n",
      "dtypes: float32(67), float64(27)\n",
      "memory usage: 84.9 MB\n"
     ]
    }
   ],
   "source": [
    "ma_feat_engine = ForexFeatureEngine(pair_name=symbol)\n",
    "ma_feat_time = ma_feat_engine.calculate_all_features(ma_time_bars, ma_timeframe, lr_period=(5, 20))\n",
    "ma_feat_time.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef1de34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0. ma_10\n",
      "  1. ma_20\n",
      "  2. ma_50\n",
      "  3. ma_100\n",
      "  4. ma_200\n",
      "  5. ma_10_20_cross\n",
      "  6. ma_20_50_cross\n",
      "  7. ma_50_200_cross\n",
      "  8. ma_spread_10_20\n",
      "  9. ma_spread_20_50\n",
      " 10. ma_spread_50_200\n",
      " 11. ma_20_slope\n",
      " 12. ma_50_slope\n",
      " 13. price_above_ma_20\n",
      " 14. price_above_ma_50\n",
      " 15. ma_ribbon_aligned\n",
      " 16. atr_14\n",
      " 17. atr_21\n",
      " 18. atr_regime\n",
      " 19. realized_vol_10\n",
      " 20. realized_vol_20\n",
      " 21. realized_vol_50\n",
      " 22. vol_of_vol\n",
      " 23. hl_range\n",
      " 24. hl_range_ma\n",
      " 25. hl_range_regime\n",
      " 26. bb_upper\n",
      " 27. bb_lower\n",
      " 28. bb_percent\n",
      " 29. bb_bandwidth\n",
      " 30. bb_squeeze\n",
      " 31. efficiency_ratio_14\n",
      " 32. efficiency_ratio_30\n",
      " 33. adx_14\n",
      " 34. dmp_14\n",
      " 35. dmn_14\n",
      " 36. adx_trend_strength\n",
      " 37. adx_trend_direction\n",
      " 38. trend_window\n",
      " 39. trend_slope\n",
      " 40. trend_t_value\n",
      " 41. trend_rsquared\n",
      " 42. trend_ret\n",
      " 43. roc_10\n",
      " 44. roc_20\n",
      " 45. momentum_14\n",
      " 46. hh_ll_20\n",
      " 47. trend_persistence\n",
      " 48. return_skew_20\n",
      " 49. return_kurtosis_20\n",
      " 50. var_95\n",
      " 51. cvar_95\n",
      " 52. market_stress\n",
      " 53. current_drawdown\n",
      " 54. days_since_high\n",
      " 55. hour_sin_h1\n",
      " 56. hour_cos_h1\n",
      " 57. hour_sin_h2\n",
      " 58. hour_cos_h2\n",
      " 59. hour_sin_h3\n",
      " 60. hour_cos_h3\n",
      " 61. dayofweek_sin\n",
      " 62. dayofweek_cos\n",
      " 63. dayofyear_sin\n",
      " 64. dayofyear_cos\n",
      " 65. sydney_session\n",
      " 66. tokyo_session\n",
      " 67. london_session\n",
      " 68. ny_session\n",
      " 69. session_overlap\n",
      " 70. friday_ny_close\n",
      " 71. sunday_open\n",
      " 72. month_end\n",
      " 73. quarter_end\n",
      " 74. sydney_session_vol\n",
      " 75. tokyo_session_vol\n",
      " 76. london_session_vol\n",
      " 77. ny_session_vol\n",
      " 78. session_overlap_vol\n",
      " 79. friday_ny_close_vol\n",
      " 80. month_end_vol\n",
      " 81. quarter_end_vol\n",
      " 82. doji\n",
      " 83. hammer\n",
      " 84. inside_bar\n",
      " 85. outside_bar\n",
      " 86. near_recent_high\n",
      " 87. near_recent_low\n",
      " 88. fractal_trend_strength\n",
      " 89. fractal_trend_direction\n",
      " 90. fractal_ma_ratio\n",
      " 91. fractal_trend_confirmation\n",
      " 92. distance_to_fractal_resistance\n",
      " 93. distance_to_fractal_support\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(ma_feat_time):\n",
    "    print(f\"{i:>3}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863eb59",
   "metadata": {},
   "source": [
    "#### Triple-Barrier Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77596f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 12716 entries, 2018-01-03 02:45:00 to 2022-12-30 12:30:00\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   t1      12716 non-null  datetime64[ns]\n",
      " 1   trgt    12716 non-null  float64       \n",
      " 2   ret     12716 non-null  float64       \n",
      " 3   bin     12716 non-null  int8          \n",
      " 4   side    12716 non-null  int8          \n",
      "dtypes: datetime64[ns](1), float64(2), int8(2)\n",
      "memory usage: 422.2 KB\n",
      "Triple-Barrier (pt=0, sl=2, h={'days': 5}):\n",
      "\n",
      "     count  proportion\n",
      "bin                   \n",
      "0    9,109    0.716342\n",
      "1    3,607    0.283658\n",
      "\n",
      "Average Uniqueness: 0.0668\n"
     ]
    }
   ],
   "source": [
    "ma_events_tb = triple_barrier_labels(\n",
    "    close=close,\n",
    "    target=vol_target,\n",
    "    t_events=t_events,\n",
    "    pt_sl=[ma_pt_barrier, ma_sl_barrier],\n",
    "    min_ret=min_ret,\n",
    "    vertical_barrier_times=vertical_barriers,\n",
    "    side_prediction=ma_side,\n",
    "    vertical_barrier_zero=False,\n",
    "    verbose=False,\n",
    ")\n",
    "ma_events_tb_time = ma_events_tb.copy()\n",
    "ma_events_tb.info()\n",
    "\n",
    "print(f\"Triple-Barrier (pt={ma_pt_barrier}, sl={ma_sl_barrier}, h={ma_time_horizon}):\")\n",
    "value_counts_data(ma_events_tb.bin, verbose=True)\n",
    "\n",
    "weights = get_event_weights(ma_events_tb, close)\n",
    "av_uniqueness = weights['tW'].mean()\n",
    "print(f\"Average Uniqueness: {av_uniqueness:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156eb058",
   "metadata": {},
   "source": [
    "#### Meta Model - CV of Weighting Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import cpu_count\n",
    "\n",
    "# Reserve 1 CPU if you want to do something else during training, otherwise set to -1\n",
    "N_JOBS = cpu_count() - 1\n",
    "N_ESTIMATORS = 100\n",
    "random_state = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413fcb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = ma_events_tb_time.copy()\n",
    "X = ma_feat_time.reindex(cont.index)\n",
    "y = cont[\"bin\"]\n",
    "t1 = cont[\"t1\"]\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "train, test = PurgedSplit(t1, test_size).split(X)\n",
    "X_train, X_test, y_train, y_test = (\n",
    "        X.iloc[train],\n",
    "        X.iloc[test],\n",
    "        y.iloc[train],\n",
    "        y.iloc[test],\n",
    "    )\n",
    "\n",
    "cont_train = get_event_weights(cont.iloc[train], bb_df.close)\n",
    "bb_cont_train = cont_train.copy()\n",
    "\n",
    "n_splits = 5\n",
    "pct_embargo = 0.01\n",
    "cv_gen = PurgedKFold(n_splits, cont_train.t1, pct_embargo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049f1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Uniqueness in Training Set: 0.7473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['unweighted', 'uniqueness', 'return'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_u = cont_train.tW.mean()\n",
    "print(f\"Average Uniqueness in Training Set: {avg_u:.4f}\")\n",
    "\n",
    "weighting_schemes = {\n",
    "    \"unweighted\": pd.Series(1., index=cont_train.index),\n",
    "    \"uniqueness\": cont_train[\"tW\"],\n",
    "    \"return\": cont_train[\"w\"],\n",
    "    }\n",
    "\n",
    "decay_factors = [0.0, 0.25, 0.5, 0.75]\n",
    "time_decay_weights = {}\n",
    "for time_decay in decay_factors:\n",
    "    decay_w = get_weights_by_time_decay_optimized(\n",
    "                triple_barrier_events=cont,\n",
    "                close_index=close.index,\n",
    "                last_weight=time_decay,\n",
    "                linear=True,\n",
    "                av_uniqueness=cont_train[\"tW\"],\n",
    "            )\n",
    "    time_decay_weights[f\"decay_{time_decay}\"] = decay_w\n",
    "        \n",
    "# for k, v in time_decay_weights.items():\n",
    "#     if k.startswith(\"linear\"):\n",
    "#         weighting_schemes[k] = v\n",
    "\n",
    "weighting_schemes.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d931c",
   "metadata": {},
   "source": [
    "##### Selection of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d27c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'standard': RandomForestClassifier(criterion='entropy', max_depth=6,\n",
       "                        min_weight_fraction_leaf=0.05, n_jobs=3, random_state=7),\n",
       " 'balanced_subsample': RandomForestClassifier(class_weight='balanced_subsample', criterion='entropy',\n",
       "                        max_depth=6, min_weight_fraction_leaf=0.05, n_jobs=3,\n",
       "                        random_state=7),\n",
       " 'max_samples': RandomForestClassifier(criterion='entropy', max_depth=6,\n",
       "                        max_samples=0.7472647467858778,\n",
       "                        min_weight_fraction_leaf=0.05, n_jobs=3, random_state=7),\n",
       " 'combined': RandomForestClassifier(class_weight='balanced_subsample', criterion='entropy',\n",
       "                        max_depth=6, max_samples=0.7472647467858778,\n",
       "                        min_weight_fraction_leaf=0.05, n_jobs=3, random_state=7)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create multiple Random Forest configurations\n",
    "\n",
    "min_w_leaf = 0.05\n",
    "max_depth = 6\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    criterion='entropy',\n",
    "    n_estimators=N_ESTIMATORS,\n",
    "    random_state=random_state,\n",
    "    min_weight_fraction_leaf=min_w_leaf,\n",
    "    max_depth=max_depth,\n",
    "    n_jobs=N_JOBS,  # Use all available cores\n",
    "    )\n",
    "\n",
    "clf0 = rf\n",
    "clf1 = clone(rf).set_params(class_weight='balanced_subsample')\n",
    "clf2 = clone(rf).set_params(max_samples=avg_u)\n",
    "clf3 = clone(rf).set_params(max_samples=avg_u, class_weight='balanced_subsample')\n",
    "\n",
    "clfs = {k: v for k, v in zip(['standard', 'balanced_subsample', 'max_samples', 'combined'], [clf0, clf1, clf2, clf3])}\n",
    "clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ecdf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier Weighting Schemes\n",
      "return standard model achieved the best neg_log_loss score of -0.6613\n",
      "\n",
      "Weighting Scheme CV:\n",
      "                          unweighted        uniqueness            return\n",
      "standard            -0.6937 ± 0.0012  -0.6934 ± 0.0013  -0.6613 ± 0.0057\n",
      "balanced_subsample  -0.6938 ± 0.0014  -0.6935 ± 0.0015  -0.6614 ± 0.0058\n",
      "max_samples         -0.6936 ± 0.0009  -0.6934 ± 0.0012  -0.6617 ± 0.0052\n",
      "combined            -0.6938 ± 0.0008  -0.6935 ± 0.0013  -0.6617 ± 0.0052\n",
      "\n",
      "Selected Best Classifier (standard): RandomForestClassifier(criterion='entropy', max_depth=6,\n",
      "                       min_weight_fraction_leaf=0.05, n_jobs=3, random_state=7)\n"
     ]
    }
   ],
   "source": [
    "# Find what model produces best CV log loss score\n",
    "\n",
    "cv_gen = PurgedKFold(n_splits, cont_train.t1, pct_embargo)\n",
    "cv_scores_d = {k: {} for k in clfs.keys()}\n",
    "print(rf.__class__.__name__, \"Weighting Schemes\")\n",
    "all_clf_scores_df = pd.DataFrame(dtype=pd.StringDtype())\n",
    "best_models = []\n",
    "best_score, best_model, best_scheme = None, None, None\n",
    "\n",
    "for scheme, sample_weights in weighting_schemes.items():\n",
    "    for param, clf in clfs.items():\n",
    "        cv_scores = ml_cross_val_score(\n",
    "            clf, X_train, y_train, cv_gen, \n",
    "            sample_weight_train=sample_weights, \n",
    "            sample_weight_score=sample_weights,\n",
    "            scoring=\"neg_log_loss\",\n",
    "        )\n",
    "        score = cv_scores.mean()\n",
    "        cv_scores_d[param][scheme] = score\n",
    "        best_score = max(best_score, score) if best_score is not None else score\n",
    "        if score == best_score:\n",
    "            best_model = param\n",
    "            best_scheme = scheme\n",
    "        all_clf_scores_df.loc[param, scheme] = f\"{cv_scores.mean():.4f} ± {cv_scores.std():.4f}\"\n",
    "\n",
    "best_clf = clone(clfs[best_model])\n",
    "print(f\"{best_scheme} {best_model} model achieved the best neg_log_loss score of {best_score:.4f}\")\n",
    "\n",
    "print(\"\\nWeighting Scheme CV:\")\n",
    "pprint(all_clf_scores_df)\n",
    "print(f\"\\nSelected Best Classifier ({best_model}): {best_clf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b427dcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighting Scheme CV:\n",
      "                   accuracy              pwa      neg_log_loss  \\\n",
      "unweighted  0.5047 ± 0.0161  0.5085 ± 0.0171  -0.6937 ± 0.0012   \n",
      "uniqueness  0.5094 ± 0.0184  0.5133 ± 0.0147  -0.6934 ± 0.0013   \n",
      "return      0.6249 ± 0.0146  0.6343 ± 0.0139  -0.6613 ± 0.0057   \n",
      "\n",
      "                  precision           recall               f1  \n",
      "unweighted  0.4963 ± 0.0307  0.3294 ± 0.0785  0.3893 ± 0.0475  \n",
      "uniqueness  0.5027 ± 0.0407  0.3409 ± 0.0925  0.3972 ± 0.0626  \n",
      "return      0.6183 ± 0.0177  0.6025 ± 0.0327  0.6101 ± 0.0247  \n",
      "\n",
      "return model achieved the best neg_log_loss score of -0.6613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze all CV scores for all weighting schemes with the best model\n",
    "\n",
    "from afml.cross_validation.cross_validation import analyze_cross_val_scores\n",
    "\n",
    "all_cv_scores_d = {}\n",
    "all_cms = {}\n",
    "best_score, best_model = None, None\n",
    "all_cv_scores_df = pd.DataFrame(dtype=pd.StringDtype())\n",
    "scoring = 'f1' if set(y_train.unique()) == {0, 1} else 'neg_log_loss'\n",
    "\n",
    "for scheme, sample_weights in weighting_schemes.items():\n",
    "    cv_scores, cv_scores_df, cms = analyze_cross_val_scores(\n",
    "        best_clf, X_train, y_train, cv_gen, \n",
    "        sample_weight_train=sample_weights, \n",
    "        sample_weight_score=sample_weights,\n",
    "    )\n",
    "    score = cv_scores[scoring].mean()\n",
    "    all_cv_scores_d[scheme] = cv_scores\n",
    "    all_cms[scheme] = cms\n",
    "    best_score = max(best_score, score) if best_score is not None else score\n",
    "    if score == best_score:\n",
    "        best_scheme = scheme\n",
    "    for idx, row in cv_scores_df.iterrows():\n",
    "        all_cv_scores_df.loc[idx, scheme] = f\"{row['mean']:.4f} ± {row['std']:.4f}\"\n",
    "\n",
    "print(\"Weighting Scheme CV:\")\n",
    "pprint(all_cv_scores_df.T)\n",
    "print(f\"\\n{best_scheme} model achieved the best {scoring} score of {best_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822b84f",
   "metadata": {},
   "source": [
    "Test if time-decay improves performance of best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f281c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Weighting Scheme CV - Return:\n",
      "                        return  return_decay_0.0 return_decay_0.25  \\\n",
      "accuracy       0.6249 ± 0.0146   0.6236 ± 0.0117   0.6236 ± 0.0120   \n",
      "pwa            0.6343 ± 0.0139   0.6332 ± 0.0144   0.6331 ± 0.0138   \n",
      "neg_log_loss  -0.6613 ± 0.0057  -0.6628 ± 0.0057  -0.6623 ± 0.0056   \n",
      "precision      0.6183 ± 0.0177   0.6184 ± 0.0162   0.6179 ± 0.0169   \n",
      "recall         0.6025 ± 0.0327   0.5966 ± 0.0310   0.5983 ± 0.0317   \n",
      "f1             0.6101 ± 0.0247   0.6071 ± 0.0227   0.6077 ± 0.0232   \n",
      "\n",
      "              return_decay_0.5 return_decay_0.75  \n",
      "accuracy       0.6224 ± 0.0121   0.6225 ± 0.0152  \n",
      "pwa            0.6336 ± 0.0137   0.6328 ± 0.0143  \n",
      "neg_log_loss  -0.6618 ± 0.0055  -0.6621 ± 0.0058  \n",
      "precision      0.6159 ± 0.0176   0.6156 ± 0.0191  \n",
      "recall         0.5999 ± 0.0297   0.6007 ± 0.0335  \n",
      "f1             0.6076 ± 0.0225   0.6079 ± 0.0257  \n",
      "\n",
      "return model achieved the best neg_log_loss score of -0.6613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model_decay_cv_scores = all_cv_scores_df[[best_scheme]]\n",
    "\n",
    "for scheme, decay_factor in time_decay_weights.items():\n",
    "    sample_weights = weighting_schemes[best_scheme] * decay_factor\n",
    "    cv_scores, cv_scores_df, cms = analyze_cross_val_scores(\n",
    "        best_clf, X_train, y_train, cv_gen, \n",
    "        sample_weight_train=sample_weights, \n",
    "        sample_weight_score=sample_weights,\n",
    "    )\n",
    "    score = cv_scores[scoring].mean()\n",
    "    scheme = f\"{best_scheme}_{scheme}\"\n",
    "    all_cv_scores_d[scheme] = cv_scores\n",
    "    all_cms[scheme] = cms\n",
    "    best_score = max(best_score, score) if best_score is not None else score\n",
    "    for idx, row in cv_scores_df.iterrows():\n",
    "        best_model_decay_cv_scores.loc[idx, scheme] = f\"{row['mean']:.4f} ± {row['std']:.4f}\"\n",
    "    if score == best_score:\n",
    "        best_scheme = scheme\n",
    "        weighting_schemes[best_scheme] = sample_weights\n",
    "        all_cv_scores_df[scheme] = best_model_decay_cv_scores[scheme]\n",
    "        \n",
    "\n",
    "print(f\"\\nBest Weighting Scheme CV - {best_scheme.title()}:\")\n",
    "pprint(best_model_decay_cv_scores)\n",
    "\n",
    "print(f\"\\n{best_scheme} model achieved the best {scoring} score of {best_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7cba8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m all_cms[scheme] \u001b[38;5;241m=\u001b[39m cms\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# pprint(all_cms, sort_dicts=False)\u001b[39;00m\n",
      "\u001b[1;32m----> 3\u001b[0m pprint(\u001b[43mall_cms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m]\u001b[49m, sort_dicts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\n",
      "\u001b[1;31mKeyError\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "all_cms[scheme] = cms\n",
    "# pprint(all_cms, sort_dicts=False)\n",
    "pprint(all_cms[best_model], sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1998e0c1",
   "metadata": {},
   "source": [
    "##### Sequential Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de198a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequentiallyBootstrappedBaggingClassifier(estimator=RandomForestClassifier(bootstrap=False,\n",
      "                                                                           criterion='entropy',\n",
      "                                                                           max_depth=6,\n",
      "                                                                           min_weight_fraction_leaf=0.05,\n",
      "                                                                           n_estimators=1,\n",
      "                                                                           n_jobs=3),\n",
      "                                          max_features=1, n_estimators=100,\n",
      "                                          n_jobs=3, oob_score=True,\n",
      "                                          price_bars_index=DatetimeIndex(['2018-01-01 23:05:00', '2018-01-01 23:10:00',\n",
      "               '2018-01-01 23:15:00', '2018-01-01 23:20:00',...\n",
      "2018-01-03 01:30:00   2018-01-03 01:50:00\n",
      "2018-01-03 02:40:00   2018-01-03 04:00:00\n",
      "2018-01-03 05:35:00   2018-01-03 08:10:00\n",
      "2018-01-03 06:45:00   2018-01-03 07:55:00\n",
      "                              ...        \n",
      "2022-10-04 14:50:00   2022-10-04 17:30:00\n",
      "2022-10-05 02:10:00   2022-10-05 04:45:00\n",
      "2022-10-05 02:40:00   2022-10-05 09:20:00\n",
      "2022-10-05 14:50:00   2022-10-05 16:20:00\n",
      "2022-10-05 16:05:00   2022-10-06 01:55:00\n",
      "Name: t1, Length: 8082, dtype: datetime64[ns],\n",
      "                                          verbose=False)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[51], line 19\u001b[0m\n",
      "\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(seq_rf)\n",
      "\u001b[0;32m     18\u001b[0m w \u001b[38;5;241m=\u001b[39m weighting_schemes[best_scheme]\n",
      "\u001b[1;32m---> 19\u001b[0m cv_scores, cv_scores_df, cms \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_cross_val_scores\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_rf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     24\u001b[0m all_cms[scheme] \u001b[38;5;241m=\u001b[39m cms\n",
      "\u001b[0;32m     26\u001b[0m scheme \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_bootstrap\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\n",
      "File \u001b[1;32mc:\\users\\joen\\documents\\github\\machine-learning-blueprint\\afml\\cross_validation\\cross_validation.py:391\u001b[0m, in \u001b[0;36manalyze_cross_val_scores\u001b[1;34m(classifier, X, y, cv_gen, sample_weight_train, sample_weight_score)\u001b[0m\n",
      "\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seq_bootstrap:\n",
      "\u001b[0;32m    388\u001b[0m     classifier \u001b[38;5;241m=\u001b[39m clone(classifier)\u001b[38;5;241m.\u001b[39mset_params(\n",
      "\u001b[0;32m    389\u001b[0m         samples_info_sets\u001b[38;5;241m=\u001b[39mt1\u001b[38;5;241m.\u001b[39miloc[train]\n",
      "\u001b[0;32m    390\u001b[0m     )  \u001b[38;5;66;03m# Create new instance\u001b[39;00m\n",
      "\u001b[1;32m--> 391\u001b[0m fit \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    395\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    396\u001b[0m prob \u001b[38;5;241m=\u001b[39m fit\u001b[38;5;241m.\u001b[39mpredict_proba(X\u001b[38;5;241m.\u001b[39miloc[test, :])\n",
      "\u001b[0;32m    397\u001b[0m pred \u001b[38;5;241m=\u001b[39m fit\u001b[38;5;241m.\u001b[39mpredict(X\u001b[38;5;241m.\u001b[39miloc[test, :])\n",
      "\n",
      "File \u001b[1;32mc:\\users\\joen\\documents\\github\\machine-learning-blueprint\\afml\\ensemble\\sb_bagging.py:323\u001b[0m, in \u001b[0;36mSequentiallyBootstrappedBaseBagging.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n",
      "\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[0;32m    303\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m    Build a Sequentially Bootstrapped Bagging ensemble of estimators from the training\u001b[39;00m\n",
      "\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    set (X, y).\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    self : (object)\u001b[39;00m\n",
      "\u001b[0;32m    322\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m--> 323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\users\\joen\\documents\\github\\machine-learning-blueprint\\afml\\ensemble\\sb_bagging.py:570\u001b[0m, in \u001b[0;36mSequentiallyBootstrappedBaggingClassifier._fit\u001b[1;34m(self, X, y, max_samples, sample_weight)\u001b[0m\n",
      "\u001b[0;32m    567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# Call parent _fit method\u001b[39;00m\n",
      "\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\users\\joen\\documents\\github\\machine-learning-blueprint\\afml\\ensemble\\sb_bagging.py:433\u001b[0m, in \u001b[0;36mSequentiallyBootstrappedBaseBagging._fit\u001b[1;34m(self, X, y, max_samples, sample_weight)\u001b[0m\n",
      "\u001b[0;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds, seeds])\n",
      "\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# Build estimators in parallel\u001b[39;00m\n",
      "\u001b[1;32m--> 433\u001b[0m all_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_estimators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_indices_\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_n_estimators\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    446\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# Unpack results\u001b[39;00m\n",
      "\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m all_results:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\JoeN\\miniforge3\\envs\\mlfinlab_env\\lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n",
      "\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n",
      "\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n",
      "\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n",
      "\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n",
      "\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n",
      "\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\JoeN\\miniforge3\\envs\\mlfinlab_env\\lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n",
      "\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n",
      "\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n",
      "\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n",
      "\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n",
      "\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n",
      "\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n",
      "\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\JoeN\\miniforge3\\envs\\mlfinlab_env\\lib\\site-packages\\joblib\\parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n",
      "\u001b[0;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n",
      "\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n",
      "\u001b[0;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n",
      "\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n",
      "\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n",
      "\u001b[0;32m   1799\u001b[0m     ):\n",
      "\u001b[1;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n",
      "\u001b[0;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n",
      "\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Base estimator for use with sequential bootstrapping\n",
    "# I chose it beacause the default behaviour of RF is to set max_features='sqrt'\n",
    "base_rf = clone(best_clf).set_params(bootstrap=False, n_estimators=1, random_state=None, n_jobs=1)\n",
    "\n",
    "seq_rf = SequentiallyBootstrappedBaggingClassifier(\n",
    "    samples_info_sets=cont_train.t1,\n",
    "    price_bars_index=bb_df.index,\n",
    "    estimator=base_rf,\n",
    "    n_estimators=20, # set low to save time\n",
    "    max_features=1,\n",
    "    max_samples=1,\n",
    "    bootstrap_features=True,\n",
    "    oob_score=True,\n",
    "    n_jobs=N_JOBS,\n",
    "    random_state=random_state,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "w = weighting_schemes[best_scheme]\n",
    "cv_scores, cv_scores_df, cms = analyze_cross_val_scores(\n",
    "        seq_rf, X_train, y_train, cv_gen, \n",
    "        sample_weight_train=w, \n",
    "        sample_weight_score=w,\n",
    "    )\n",
    "all_cms[scheme] = cms\n",
    "\n",
    "scheme = 'seq_bootstrap'\n",
    "for idx, row in cv_scores_df.iterrows():\n",
    "    all_cv_scores_df.loc[idx, scheme] = f\"{row['mean']:.4f} ± {row['std']:.4f}\"\n",
    "\n",
    "all_cv_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e48f7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Bootstrap done in 0 days 00:12:06\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "standard_rf",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sequential_rf",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sequential_rf_avgu",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sequential_rf_unweighted_avgu",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "7ed9ac37-e0c2-4db8-9da8-b21aea2df33a",
       "rows": [
        [
         "accuracy",
         "0.4855",
         "0.504",
         "0.5013",
         "0.5"
        ],
        [
         "pwa",
         "0.4918",
         "0.5024",
         "0.4899",
         "0.4916"
        ],
        [
         "neg_log_loss",
         "-0.7232",
         "-0.7016",
         "-0.7036",
         "-0.7033"
        ],
        [
         "precision",
         "0.4888",
         "0.5077",
         "0.5054",
         "0.5038"
        ],
        [
         "recall",
         "0.4852",
         "0.4728",
         "0.4315",
         "0.4315"
        ],
        [
         "f1",
         "0.487",
         "0.4896",
         "0.4655",
         "0.4649"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standard_rf</th>\n",
       "      <th>sequential_rf</th>\n",
       "      <th>sequential_rf_avgu</th>\n",
       "      <th>sequential_rf_unweighted_avgu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.4855</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.5013</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pwa</th>\n",
       "      <td>0.4918</td>\n",
       "      <td>0.5024</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.4916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_log_loss</th>\n",
       "      <td>-0.7232</td>\n",
       "      <td>-0.7016</td>\n",
       "      <td>-0.7036</td>\n",
       "      <td>-0.7033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.4888</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.5054</td>\n",
       "      <td>0.5038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.4852</td>\n",
       "      <td>0.4728</td>\n",
       "      <td>0.4315</td>\n",
       "      <td>0.4315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.4870</td>\n",
       "      <td>0.4896</td>\n",
       "      <td>0.4655</td>\n",
       "      <td>0.4649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              standard_rf  sequential_rf  sequential_rf_avgu  \\\n",
       "accuracy           0.4855         0.5040              0.5013   \n",
       "pwa                0.4918         0.5024              0.4899   \n",
       "neg_log_loss      -0.7232        -0.7016             -0.7036   \n",
       "precision          0.4888         0.5077              0.5054   \n",
       "recall             0.4852         0.4728              0.4315   \n",
       "f1                 0.4870         0.4896              0.4655   \n",
       "\n",
       "              sequential_rf_unweighted_avgu  \n",
       "accuracy                             0.5000  \n",
       "pwa                                  0.4916  \n",
       "neg_log_loss                        -0.7033  \n",
       "precision                            0.5038  \n",
       "recall                               0.4315  \n",
       "f1                                   0.4649  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = weighting_schemes[best_scheme]\n",
    "rf = best_clf.set_params(oob_score=True).fit(\n",
    "    X_train, y_train, sample_weight=w,\n",
    ")\n",
    "\n",
    "time0 = time.time()\n",
    "seq_rf.set_params(oob_score=True).fit(\n",
    "    X_train, y_train, sample_weight=w,\n",
    ")\n",
    "time1 = pd.Timedelta(seconds=time.time() - time0).round('1s')\n",
    "print(f\"Sequential Bootstrap done in {time1}\")\n",
    "\n",
    "ensembles = {\n",
    "    \"standard_rf\": {\"classifier\": rf, \n",
    "                    \"pred\": rf.predict(X_test),\n",
    "                    \"prob\": rf.predict_proba(X_test),\n",
    "                    \"oob\": rf.oob_score_,\n",
    "                },\n",
    "    \"sequential_rf\": {\"classifier\": seq_rf, \n",
    "                      \"pred\": seq_rf.predict(X_test),\n",
    "                      \"prob\": seq_rf.predict_proba(X_test),\n",
    "                      \"oob\": seq_rf.oob_score_,\n",
    "                      },\n",
    "}\n",
    "\n",
    "scoring_methods = {\n",
    "            \"accuracy\": accuracy_score,\n",
    "            \"pwa\": probability_weighted_accuracy,\n",
    "            \"neg_log_loss\": log_loss,\n",
    "            \"precision\": precision_score,\n",
    "            \"recall\": recall_score,\n",
    "            \"f1\": f1_score,\n",
    "        }\n",
    "\n",
    "all_scores_oos = pd.DataFrame()\n",
    "\n",
    "for clf in ensembles.keys():\n",
    "    for method, scoring in scoring_methods.items():\n",
    "        if scoring in (probability_weighted_accuracy, log_loss):\n",
    "            y_pred = ensembles[clf][\"prob\"]\n",
    "        else:\n",
    "            y_pred = ensembles[clf][\"pred\"]\n",
    "        score = scoring(y_test, y_pred)\n",
    "        if method == \"neg_log_loss\":\n",
    "            score *= -1\n",
    "        all_scores_oos.loc[method, clf] = score\n",
    "    \n",
    "all_scores_oos.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550ae1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "winsound.Beep(1000, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlfinlab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
